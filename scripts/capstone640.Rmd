---
title             : "EDLD 640 Capstone"
shorttitle        : "Natural Language Processing for Pedagogy"
author: 
  - name          : "Diana DeWald"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "336 Straub Hall University of Oregon, Eugene OR, 97403"
    email         : "ddewald@uoregon.edu"
  - name          : "Dare Baldwin"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "University of Oregon"

abstract: |

  (will change wording to past tense once project is completed, and will add in results and discussion summary).. 
  
  How is young children's exploration impacted by adult pedagogy? Can we create Machine Learning models to predict how a child will explore the causal features of an object based upon the pedagogy they are exposed to? Our goal is to establish predictive models of preschoolers' causal learning outcomes within educational settings based upon teachers' pedagogical styles. Using pre-existing samples of pedagogy and child outcomes, we...
  
  The rationale for this investigation is that determining the pedagogy-inclusive models predicting children's behavior in educational settings will allow us to predict cases where adult-directed instruction creates positive learning outcomes.
  
  
  <!-- https://tinyurl.com/ybremelq -->
keywords          : "causal learning models, pedagogy, text analysis"
wordcount         : "944"
bibliography      : ["r-references.bib"]
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
---

```{r loading libraries, include = FALSE, warning= FALSE}
library(papaja)
r_refs("r-references.bib")
library(tidyverse)
library(here)
library(rio)
library(tidytext)
library(dplyr)
library(tidyr)

library(devtools)
library(tinytex)
library(readr)
library(magrittr)
library(recipes)
library(psych)
library(finalfit)
library(caret)
library(glmnet)
library(recipes)
library(cutpointr)
library(kableExtra)
library(pastecs)



```

# Introduction
  Developmentalists and educators have long debated the benefits of child-directed (Montessori style) exploration contra adult-directed (pedagogical) instruction on learning outcomes. The truth is that both provide valuable learning opportunities, but questions of how, in what cases, and for which individuals remain fuzzy. While young children (age 3-6) often re-structure their hypotheses about the world based upon self-directed exploration á la Montessori, there are many subjects children cannot master without adult-directed pedagogical guidance (e.g., novel object labels, the alphabet, color and shape labels, historical events, the existence of entities such as germs, etc.). Such subjects are often culturally and linguistically bound, but even causal learning related to the physical properties of objects and entities can benefit from adult-directed pedagogy. Clarifying the extent to which pedagogy supports—and in some cases diminishes—effective causal learning is essential for a) informing teaching strategies in a time when many preparatory schools in the U.S. suffer from a lack of funding and teacher support [@SRCD; @NIERR], and b) elevating early education outcomes following relative dips in school preparedness over the past three years (Jalongo, 2021; [@gonzalez2022school]).
  In early childhood (age 3-6), children learn about causality through both self-directed and adult-directed methods. In recent years, adult-directed early education programs have undergone substantial changes in the U.S., yet the vast majority of causal learning models remain focused on child-directed learning outcomes. Those who have sought to address the impact of adult-directed pedagogy on causal learning describe a pedagogical trade-off model. This model proposes that adult instruction increases the proportion of time children spend exploring an object’s pedagogy-relevant properties but limits their investigation of other properties. Conversely, child-directed exploration is understood to produce broader discovery of the complete set of an object’s causal properties but diminish the time spent investigating any particular property. While such behavioral outcomes are established, little is known about the cognitive mechanisms that drive this pedagogical trade-off, how to computationally map the trade-off, and the extent to which computational models capture individual differences in learning outcomes. Failing to assess the differential impact of pedagogy on causal learning during early childhood limits educators to theories that only take child-directed learning into account. 
  While computational models of children’s causal learning exist (e.g., Kosoy et al., 2022; Gopnik et al. 2004; Sobel, 2014; Oudeyer & Smith, 2014; Twomey & Westermann, 2016; Bonawitz et al., 2022; Colantonio et al., in press), there is no pre-existing model which factors in pedagogy-type as a predictor of learning behaviors. By utilizing machine learning to train models both with-and-without the presence of diverse pedagogy categories, we can greatly expand the predictive power of causal learning models, which are currently limited to child-directed learning predictors. 
  Our long-term goal is to establish predictive models of preschoolers’ causal learning outcomes within naturalistic settings based upon teachers’ pedagogical styles. The overall objective is to elucidate how interactions between pedagogy type, attentional patterns, and exploratory behavior inform competing computational models of causal learning outcomes and to train a best-performing model via machine learning. Our central hypotheses is as follows: causal learning models will perform best when taking granular pedagogy types into account. We aim to create and test competing computational models related to the interaction between pedagogy type and causal learning. We predict that prior computational models of causal learning that contain fewer pedagogy categories (or do not take pedagogy into account) will perform worse than pedagogy-diverse models of causal learning. Upon completion, our expected outcomes are to have established the interaction between adult instruction style and children’s visual attention processes and exploratory behaviors within physical causal learning domains. These results will a) add valuable evidence to clarify developmentalists’ theoretical and computational accounts of causal learning, and b) pave a way forward to support educators in developing effective curriculum for young students during a time of immense educational resource shortages.

# Methods
  In Fall of 2022, we conducted a survey of undergraduate students (N = 168) at the University of Oregon, asking participants to report how they would teach a child about the causal properties of a novel object. Participants were randomly sorted into 2 conditions. In the 'enhance' condition, participants were asked to generate pedagogy for two object properties intended to produce broad exploratory behaviors from a child (ex: "What would you say to introduce this toy in a way that encourages wide-ranging exploration?"). In the 'constrain' condition, participants were asked to generate pedagogy intended to produce limited exploratory behaviors from a child (ex: "What would you say to introduce this toy in a way that discourages wide-ranging exploration?"). 
  
  We then (in the works) created a coding classifier system pairing participant-generated pedagogy with 7 pedagogy-type categories from previous research. These 7 pedagogy categories were linked to specific child outcomes from prior studies conducted by us as well as other labs. Using our child data outcome data paired with the coded participant-generated pedagogy, we are creating models to capture how well participants in our survey generated pedagogy which was most likely to produce the desired child outcomes.

### outline: describe models (see whether bagged tree or random forest model better predicts the sentiment of pedagogy--use this to see if some pedagogy sentiment groups are tied to certain child outcomes), coding system, variables, etc.

### Coding system: based on rank order created for gen survey


```{r data cleaning, warning = FALSE}

# loading datasets

mydata <- import(here("data", "pedagogy_data.xlsx"))
textdata <- import(here("data", "text_data.xlsx"))
# devdata <-

# fixing demographic variables


# pivoting longer
mydata <- mydata %>%
  pivot_longer(cols = starts_with("f"), 
               names_to = "func", 
               values_to = "pedagogy")

# renaming two variables in funct column and getting rid of old func column
fun <- c(fsqueak = "squeak", flight = "light")

mydata$funct <- 
  as.character(fun[mydata$func])

mydata$func <- NULL
rm(fun)

```

```{r initial plots, warning = FALSE}

# parsing words from the 'pedagogy' (text) column
tidy_words <- mydata %>%
  unnest_tokens(word, pedagogy)

# removing numbers
tidy_words <- tidy_words[-grep("\\b\\d+\\b", tidy_words$word),]

# removing common/under-informative words
exclu <- tibble(word = c("the", "this", "I"))

tidy_words <- tidy_words %>%
  anti_join(exclu, by = "word")


#plot
tidy_words %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = reorder(word, n)) %>% # make y-axis ordered by n
  slice(1:15) %>% # select only the first 15 rows
  ggplot(aes(n, word)) +
  geom_col(fill = "royalblue", alpha = .7) +
  scale_x_continuous(expand = c(0,0)) +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.x = element_line(color = "gray80")
  ) +
  labs(
    x = "Word Frequency",
    y = "Word",
    title = "Top 15 most frequently occurring words across all pedagogy types",
  )

```


```{r analysis-preferences, include = FALSE, warning = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


```{r nlp start, warning = FALSE}

# tokenizing
unigrams = textdata %>% unnest_tokens(word, text, token = "ngrams", n = 1)
bigrams = textdata %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)

# no stop words
unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)

# stemming
library(SnowballC)

unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)%>%
  mutate(word = wordStem(word))

# lemmatization
library(textstem)

unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)%>%
  mutate(word = lemmatize_words(word))

# cleaning n-grams

library(tm)

ngramsclean <- textdata %>%
  unnest_tokens(word, text, token = "ngrams", n = 1)%>%
  filter(!word %in% stop_words$word)%>% # removing stop words
  mutate(word = lemmatize_words(word))%>% 
  group_by(id)%>%
  summarise(text = paste(word, collapse = " "))%>%
  unnest_tokens(word, text, token = "ngrams", n = 3)

```

### Wordcloud for overall sentiment

```{r, warning = FALSE}
# visualing: word cloud
library(wordcloud)

tokens = textdata %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

# top words
word_count = tokens%>%
  group_by(word)%>%
  summarise(count = n())%>%
  arrange(desc(count))%>%
  slice(1:10)


# word cloud--zoom in
cloud <- tokens %>%
  group_by(source, word) %>%
  summarise(count = n())%>%
  arrange(desc(count))%>%
  slice(1:10)
  

wordcloud(tokens$word, max.words = 75, colors=brewer.pal(6, "Dark2"))

# word embeddings
library(text2vec)

# text8_file = "~/text8"

# unzip ("text8.zip", files = "text8", exdir = "~/")

# wiki = readLines(text8_file, n = 1, warn = FALSE)

# Create iterator over tokens
tokens <- space_tokenizer(unigrams_cleaned$word) 

# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)

vocab <- prune_vocabulary(vocab, term_count_min = 5L)
vectorizer <- vocab_vectorizer(vocab)

tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)


glove = GlobalVectors$new(rank = 50, x_max = 10)
# shakes_wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.001)

# wv_context = glove$components
# word_vectors = shakes_wv_main  + t(wv_context)

# berlin = word_vectors["paris", , drop = FALSE] -
#  word_vectors["france", , drop = FALSE] +
#  word_vectors["germany", , drop = FALSE]

# berlin_cos_sim = sim2(x = word_vectors, y = berlin, method = "cosine", norm = "l2")
# head(sort(berlin_cos_sim[,1], decreasing = TRUE), 5)


# topic modelling--more work to do
library(topicmodels)


```

### wordcloud for positive sentiment

```{r, warning=FALSE}
# sentiment analysis
library(RColorBrewer)


sentiment = textdata %>% #this allows us to retain the row number/the tweet
  unnest_tokens(word, text)%>% # this unnests the tweets into words
  anti_join(stop_words)%>% #removes common words (stop words)
  left_join(get_sentiments("bing"))

library(writexl)


write_xlsx(sentiment, "sentiment.xlsx")
sentimentdata <- import(here("data", "sentiment.xlsx"))

sentiment_by_condition <- sentimentdata %>%
  group_by(condition) %>%
  count(sentiment)

word_countID = tokens%>%
  group_by(id) %>%
  summarise(count = n())

sentimentdata <- left_join(sentimentdata, word_countID, by = "id")
# the enhance condition has roughly equal positive and negative sentiment, whereas the constrain condition has a ratio of ~ 1:3 positive to negative words (note: neutral words have been removed--sentiment was calculated according to "bing"--line 280 above)

# predictors for sentiment (and later, for child outcomes): word count, function (squeak or light), condition (enhance or constrain), source -->possible to add later: age, gender, ethnicity, adherence

# word cloud
positive_sentiment = sentiment%>% filter(!is.na(sentiment),
                                         sentiment == 'positive') 

wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))

```

### Wordcloud for negative sentiment

```{r, warning = FALSE}

negative_sentiment = sentiment%>% filter(!is.na(sentiment),
                                         sentiment == 'negative') 

wordcloud(negative_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
```


### Sentiment Radar Chart

```{r, warning=FALSE}
#### sentiment radar chart

library(fmsb)
library(tidyr)
library(textdata)

nrc_sentiment = textdata %>% #this allows us to retain the row number
  unnest_tokens(word, text)%>% # this unnests the tweets into words
  anti_join(stop_words)%>% #removes common words (stop words)
  left_join(get_sentiments("nrc"))%>%
  filter(!is.na(sentiment))

nrc_sentiment = nrc_sentiment%>%
  group_by(sentiment)%>%
  summarise(count = n())%>%
  spread(sentiment, count)

# create rows with the min and max to be plotted
nrc_sentiment <- rbind(rep(400,10) , rep(0,10), nrc_sentiment)

radarchart(nrc_sentiment, axistype=1 , 
           pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 , 
           cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,20,5), cglwd=0.8,
           vlcex=0.8 )

#### sentiment over time

library(gutenbergr)
library(tidyr)
library(ggplot2)


words <- textdata %>%
  mutate(linenumber = row_number())%>%
  unnest_tokens(word, text)

sentbars = words %>%
  inner_join(get_sentiments("bing"))%>%
  # %/% performs integer divison, rounding down to the nearest whole number
  mutate(index = linenumber %/% 25)%>% 
  group_by(index, sentiment)%>%
  summarise(count = n())%>%
  spread(sentiment, count, fill = 0) %>%
  mutate(sentiment = positive - negative,
         sentiment_group = ifelse(sentiment > 0, "pos", "neg"))%>%
  ungroup()

# bing - just looks at words being used compared to a pre-created dictionary; is there one that understands sentences. Find pre-defined model w/ positive or negative sentiment (positive, negative, neutral).

# the plot itself
ggplot(data = sentbars) +
  geom_bar(aes(x = index, y = sentiment, fill = sentiment_group), stat = "identity")+
  theme_classic()+
  theme(
    legend.position = "none",
    axis.ticks.x = element_blank(),
    axis.line.x = element_blank(),
    axis.text.x = element_blank()
  )+
  scale_fill_manual(values = c("darkred", "darkgreen"))+
  labs(title = "The Sentiment of Pedagogy",
       x = "Change over Time (Each Bar is 25 Lines of Text)",
       y = "Sentiment Score")

# parts of speech tagging
## load udpipe
library(udpipe)
library(kableExtra)

## download the "english" model
udmodel <- udpipe_download_model(language = "english")

##let's load the "english' model
udmodel <- udpipe_load_model(file = udmodel$file_model)


pos <- udpipe_annotate(udmodel, 
                       x = unigrams_cleaned$word)

pos <- as.data.frame(pos)

## how can get a count of words by parts of speech using the following data.
table(pos$upos)

pos <- cbind_dependencies(pos, type = "parent")
nominalsubject <- subset(pos, dep_rel %in% c("nsubj"))
nominalsubject <- nominalsubject[, c("dep_rel", "token", "token_parent")]
```

```{r, warning = FALSE}
ggplot(data = cloud)+
  geom_bar(aes(x = word, y = count, fill = source), stat = "identity", position = "dodge")+
  theme_classic()


```








## Part 1: Predicting a Categorical Outcome (sentiment: pos or neg) using Regularized Logistic Regression

```{r part1_prep, warning = FALSE}

# Recipe for the sentiment dataset

outcome <- c('sentiment')

ID <- c('id')

categorical <- c('condition', 'source', 'function', 'count')

# Problem: currently getting an error when I try to train the model.The error is coming from step_medianimpute(). step_medianimpute() requires all the variables to be numeric but it is being passed factor variables with all_predictors().One way to fix this problem is by rearranging your recipe to create dummy variables before you impute.need to dummy code

blueprint <- recipe(x  = sentimentdata,
                    vars  = c(categorical, outcome, ID),
                    roles = c(rep('predictor',4), 'outcome', 'ID')) %>%
  step_zv(all_of(categorical)) 
 
blueprint

```

### Task 1.1: split dataset into training and testing

```{r 1.1, warning = FALSE}
# Let the training data have the 80% of cases and the test data have the 20% of the cases.

set.seed(11102021)  # for reproducibility
  
sen      <- sample(1:nrow(sentimentdata), round(nrow(sentimentdata) * 0.8))
sen_train  <- sentimentdata[sen, ]
sen_test  <- sentimentdata[-sen, ]

```


### Task 1.2: 10-fold cross-validation without regularization

```{r 1.2, warning = FALSE}

set.seed(11152021) # for reproducibility

sen_tr = sen_train[sample(nrow(sen_train)),]

  # Creating 10 folds with equal size

folds = cut(seq(1,nrow(sen_tr)),breaks=10,labels=FALSE)
  
  # Creating the list for each fold 
sen.indices <- vector('list',10)

      for(i in 1:10){
        sen.indices[[i]] <- which(folds!=i)
      }
    

 cv <- trainControl(method    = "cv",
                   index           = sen.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)
    
      
# Train the model
  
caret_mod <- caret::train(blueprint, 
                          data      = sen_tr, 
                          method    = "glm",
                          family    = 'binomial',
                          metric    = 'logLoss',
                          trControl = cv)

caret_mod

# Evaluate the model on the test data

# Predict the probabilities for the observations in the test dataset

predicted_test <- predict(caret_mod, sen_test, type='prob')

head(predicted_test)


# Evaluate the model on the test dataset
predicted_te <- predict(caret_mod, sen_test)

predicted_te <- as.numeric(predicted_te)
sen_test_numeric <- sen_test$sentiment_numeric


predicted_eval <- data.frame(predicted_te, sen_test_numeric)
predicted_eval <- predicted_eval[complete.cases(predicted_eval), ]


rsq_te <- cor(predicted_eval$predicted_te,predicted_eval$sen_test_numeric)^2
rsq_te

mae_te <- mean(abs(predicted)eval$sen_test_numeric - predicted_te))
mae_te

rmse_te <- sqrt(mean((oregon_test$score - predicted_te)^2))
rmse_te

```
#### Will discuss this more in 1.5, but the model without regularization doesn't seem to be the best for predicting outcomes on the test dataset.

### Task 1.3: 10-fold cross-validation with ridge penalty
```{r 1.3 ridge, warning = FALSE}
 caret::getModelInfo()$glmnet$parameters

# currently getting this error when running ridge and lasso penalty: Error in { : 
#  task 1 failed - "error in evaluating the argument 'x' in selecting a method for function 'as.matrix': invalid class 'NA' to dup_mMatrix_as_dgeMatrix"
# In addition: There were 20 warnings (use warnings() to see them)


grid_ridge <- data.frame(alpha = 0, lambda = seq(0.01,2,.01)) 
# grid
  
  ridge <- caret::train(blueprint, 
                        data      = sen_train, 
                        method    = "glmnet",
                        family = 'binomial', 
                        metric = 'logLoss',
                        trControl = cv,
                        tuneGrid  = grid_ridge)
  
# ridge$results


# ridge$bestTune
  
# plot(ridge)


```

The optimal lambda value for a lasso penalty was _____


According to the second lasso model, the optimal lambda value for lasso penalty is _______


### Find and report the most important 10 predictors of sentiment and their coefficients. 




## Data analysis: Logistic regression (in progress)

```{r, include = TRUE, warning= FALSE, include=FALSE}

omsi <- import(here("data", "omsidata.xlsx"))

omsi$condition <- as.factor(omsi$condition)

outcome <- c('squeaker_discovered')

ID <- c('participant')

categorical <- c('condition')

# Problem: currently getting an error when I try to train the model.The error is coming from step_medianimpute(). step_medianimpute() requires all the variables to be numeric but it is being passed factor variables with all_predictors().One way to fix this problem is by rearranging your recipe to create dummy variables before you impute.need to dummy code

blueprint <- recipe(x  = omsi,
                    vars  = c(categorical, outcome, ID),
                    roles = c(rep('predictor'),'outcome','ID')) %>%
  step_indicate_na(all_of(categorical)) %>%
  step_zv(all_of(categorical)) %>%
    step_num2factor(outcome,
                  transform = function(x) x + 1,
                  levels=c('Y', 'N')) %>%
  step_num2factor(condition, 
                  transform = function(x) x + 1,
                  levels=c('ped1', 'baseline', 'interrupted', 'naive'))
 
blueprint


# let's split into testing and training sets
# Let the training data have the 80% of cases and the test data have the 20% of the cases.

set.seed(11102021)  # for reproducibility
  
ped     <- sample(1:nrow(omsi), round(nrow(omsi) * 0.8))
ped_train  <- omsi[ped, ]
ped_test  <- omsi[-ped, ]


# 10 fold cross validation


set.seed(11152021) # for reproducibility

omsi_tr = ped_train[sample(nrow(ped_train)),]

  # Creating 10 folds with equal size (creating an empty list not currently working)

folds_omsi = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
  
# code currently providing error when I try to create an empty list
  # Creating the list for each fold 
# omsi_indices <- vector(mode = 'list',length = 10)
#      for(i in 1:10){
#        omsi_indices[[i]] <- which(folds_omsi!=i)
#      }
    
set.seed(125)

 cv <- trainControl(method    = "cv",
                      classProbs      = TRUE,
                   summaryFunction = mnLogLoss,
                   number = 10)
 
    
      
# Train the model
  
 ped_mod <- caret::train(blueprint, 
                          data      = omsi_tr, 
                          method    = "glm",
                          family    = 'binomial',
                          metric    = 'logLoss',
                          trControl = cv)
rlang::last_error()
# ped_mod

# Evaluate the model on the test data

# Predict the probabilities for the observations in the test dataset

# predicted_test <- predict(ped_mod, omsi_test, type='prob')

# head(predicted_test)

```

## K-nearest neighbors algorithm to predict rank ordered exploration-promotion

```{r}
# import data
textdummy <- import(here("data", "text_data_dummy.xlsx"))

# train and test split

set.seed(10152022) # for reproducibility

loc <- sample(1:nrow(textdummy), round(nrow(textdummy) * 0.9))
rank_tr <- textdummy[loc, ]
rank_te <- textdummy[-loc, ]


# create row indices for 10-folds

  #randomly shuffle training data
rank_tr = rank_tr[sample(nrow(rank_tr)),]


    # Create 10 folds with equal size

      folds = cut(seq(1,nrow(rank_tr)),breaks=10,labels=FALSE)
  
    # Create the list for each fold 
      
      my.indices <- vector('list',10)
      for(i in 1:10){
        my.indices[[i]] <- which(folds!=i)
      }

# Cross-validation settings
      
  cv <- trainControl(method = "cv",
                     index  = my.indices)
  
require(caret)
require(kknn)

getModelInfo()$kknn$parameters


# Hyperparameter Tuning Grid
  
  grid <- expand.grid(kmax    = 3:25,
                     distance = c(1,2,3),
                     kernel   = c('epanechnikov','rectangular'))
  grid
  
  
require(doParallel)

ncores <- 8    

cl <- makePSOCKcluster(ncores)

registerDoParallel(cl)


# Train the model


outcome <- c('rank')

ID <- c('id')

categorical <- c('condition', 'source', 'function')


blueprint_textdummy <- recipe(x  = textdummy,
                    vars  = c(categorical, outcome, ID),
                    roles = c(rep('predictor',3), 'outcome', 'ID')) 


  caret_knn <- caret::train(blueprint_textdummy, 
                                        data      = rank_tr,
                                        method    = "kknn",
                                        trControl = cv,
                                        tuneGrid  = grid)

```


# Results (in progress)

# Discussion (in progress)


\newpage

# References
We used packages from `r cite_r("r-references.bib")` for all our analyses.

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
