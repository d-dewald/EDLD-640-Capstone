---
title             : "EDLD 640 Capstone"
shorttitle        : "Natural Language Processing for Pedagogy"
author: 
  - name          : "Diana DeWald"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "336 Straub Hall University of Oregon, Eugene OR, 97403"
    email         : "ddewald@uoregon.edu"
  - name          : "Dare Baldwin"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "University of Oregon"
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  
  <!-- https://tinyurl.com/ybremelq -->
keywords          : "keywords"
wordcount         : "X"
bibliography      : ["r-references.bib"]
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
---

```{r loading libraries, include = FALSE}
library(papaja)
r_refs("r-references.bib")
library(tidyverse)
library(here)
library(rio)
library(tidytext)
library(dplyr)
library(tidyr)



```

```{r data cleaning, include = FALSE}

# loading datasets

mydata <- import(here("data", "pedagogy_data.xlsx"))
textdata <- import(here("data", "text_data.xlsx"))
# devdata <-

# fixing demographic variables


# pivoting longer
mydata <- mydata %>%
  pivot_longer(cols = starts_with("f"), 
               names_to = "func", 
               values_to = "pedagogy")

# renaming two variables in funct column and getting rid of old func column
fun <- c(fsqueak = "squeak", flight = "light")

mydata$funct <- 
  as.character(fun[mydata$func])

mydata$func <- NULL
rm(fun)

```

```{r initial plots, include=FALSE}

# parsing words from the 'pedagogy' (text) column
tidy_words <- mydata %>%
  unnest_tokens(word, pedagogy)

# removing numbers
tidy_words <- tidy_words[-grep("\\b\\d+\\b", tidy_words$word),]

# removing common/under-informative words
exclu <- tibble(word = c("the", "this", "I"))

tidy_words <- tidy_words %>%
  anti_join(exclu, by = "word")


#plot
tidy_words %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = reorder(word, n)) %>% # make y-axis ordered by n
  slice(1:15) %>% # select only the first 15 rows
  ggplot(aes(n, word)) +
  geom_col(fill = "royalblue", alpha = .7) +
  scale_x_continuous(expand = c(0,0)) +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.x = element_line(color = "gray80")
  ) +
  labs(
    x = "Word Frequency",
    y = "Word",
    title = "Top 15 most frequently occurring words across all pedagogy types",
  )

```


```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


```{r nlp start}

# tokenizing
unigrams = textdata %>% unnest_tokens(word, text, token = "ngrams", n = 1)
bigrams = textdata %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)

# no stop words
unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)

# stemming
library(SnowballC)

unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)%>%
  mutate(word = wordStem(word))

# lemmatization
library(textstem)

unigrams_cleaned = unigrams %>%
  filter(!word %in% stop_words$word)%>%
  mutate(word = lemmatize_words(word))

# cleaning n-grams

library(tm)

ngramsclean <- textdata %>%
  unnest_tokens(word, text, token = "ngrams", n = 1)%>%
  filter(!word %in% stop_words$word)%>% # removing stop words
  mutate(word = lemmatize_words(word))%>% 
  group_by(id)%>%
  summarise(text = paste(word, collapse = " "))%>%
  unnest_tokens(word, text, token = "ngrams", n = 3)


# visualing: word cloud
library(wordcloud)

tokens = textdata %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

# top words
word_count = tokens%>%
  group_by(word)%>%
  summarise(count = n())%>%
  arrange(desc(count))%>%
  slice(1:10)


# word cloud--zoom in
cloud <- tokens %>%
  group_by(source, word) %>%
  summarise(count = n())%>%
  arrange(desc(count))%>%
  slice(1:10)


ggplot(data = cloud)+
  geom_bar(aes(x = word, y = count, fill = source), stat = "identity", position = "dodge")+
  theme_classic()
  

wordcloud(tokens$word, max.words = 75, colors=brewer.pal(6, "Dark2"))

# word embeddings
library(text2vec)

text8_file = "~/text8"

unzip ("text8.zip", files = "text8", exdir = "~/")

wiki = readLines(text8_file, n = 1, warn = FALSE)

# Create iterator over tokens
tokens <- space_tokenizer(unigrams_cleaned$word) 

# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)

vocab <- prune_vocabulary(vocab, term_count_min = 5L)
vectorizer <- vocab_vectorizer(vocab)

tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)


glove = GlobalVectors$new(rank = 50, x_max = 10)
# shakes_wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.001)

# wv_context = glove$components
# word_vectors = shakes_wv_main  + t(wv_context)

# berlin = word_vectors["paris", , drop = FALSE] -
#  word_vectors["france", , drop = FALSE] +
#  word_vectors["germany", , drop = FALSE]

# berlin_cos_sim = sim2(x = word_vectors, y = berlin, method = "cosine", norm = "l2")
# head(sort(berlin_cos_sim[,1], decreasing = TRUE), 5)


# topic modelling--more work to do
library(topicmodels)







# sentiment analysis
library(RColorBrewer)


sentiment = textdata %>% #this allows us to retain the row number/the tweet
  unnest_tokens(word, text)%>% # this unnests the tweets into words
  anti_join(stop_words)%>% #removes common words (stop words)
  left_join(get_sentiments("bing"))

# word cloud
positive_sentiment = sentiment%>% filter(!is.na(sentiment),
                                         sentiment == 'positive') 

wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))

negative_sentiment = sentiment%>% filter(!is.na(sentiment),
                                         sentiment == 'negative') 

wordcloud(negative_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))

#### sentiment radar chart

library(fmsb)
library(tidyr)
library(textdata)

nrc_sentiment = textdata %>% #this allows us to retain the row number/the tweet
  unnest_tokens(word, text)%>% # this unnests the tweets into words
  anti_join(stop_words)%>% #removes common words (stop words)
  left_join(get_sentiments("nrc"))%>%
  filter(!is.na(sentiment))

nrc_sentiment = nrc_sentiment%>%
  group_by(sentiment)%>%
  summarise(count = n())%>%
  spread(sentiment, count)

# create rows with the min and max to be plotted
nrc_sentiment <- rbind(rep(400,10) , rep(0,10), nrc_sentiment)

radarchart(nrc_sentiment, axistype=1 , 
           pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 , 
           cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,20,5), cglwd=0.8,
           vlcex=0.8 )

#### sentiment over time

library(gutenbergr)
library(tidyr)
library(ggplot2)


words <- textdata %>%
  mutate(linenumber = row_number())%>%
  unnest_tokens(word, text)

sentbars = words %>%
  inner_join(get_sentiments("bing"))%>%
  # %/% performs integer divison, rounding down to the nearest whole number
  mutate(index = linenumber %/% 25)%>% 
  group_by(index, sentiment)%>%
  summarise(count = n())%>%
  spread(sentiment, count, fill = 0) %>%
  mutate(sentiment = positive - negative,
         sentiment_group = ifelse(sentiment > 0, "pos", "neg"))%>%
  ungroup()

# the plot itself
ggplot(data = sentbars) +
  geom_bar(aes(x = index, y = sentiment, fill = sentiment_group), stat = "identity")+
  theme_classic()+
  theme(
    legend.position = "none",
    axis.ticks.x = element_blank(),
    axis.line.x = element_blank(),
    axis.text.x = element_blank()
  )+
  scale_fill_manual(values = c("darkred", "darkgreen"))+
  labs(title = "The Sentiment of Pedagogy",
       x = "Change over Time (Each Bar is 25 Lines of Text)",
       y = "Sentiment Score")

# parts of speech tagging
## load udpipe
library(udpipe)
library(kableExtra)

## download the "english" model
udmodel <- udpipe_download_model(language = "english")

##let's load the "english' model
udmodel <- udpipe_load_model(file = udmodel$file_model)


pos <- udpipe_annotate(udmodel, 
                       x = unigrams_cleaned$word)

pos <- as.data.frame(pos)

## how can get a count of words by parts of speech using the following data.
table(pos$upos)

pos <- cbind_dependencies(pos, type = "parent")
nominalsubject <- subset(pos, dep_rel %in% c("nsubj"))
nominalsubject <- nominalsubject[, c("dep_rel", "token", "token_parent")]
```

# Methods


## Participants

## Material

## Procedure

## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.


# Results

# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
