filter(!word %in% stop_words$word)
# stemming
library(SnowballC)
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)%>%
mutate(word = wordStem(word))
# lemmatization
library(textstem)
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)%>%
mutate(word = lemmatize_words(word))
# cleaning n-grams
library(tm)
ngramsclean <- textdata %>%
unnest_tokens(word, text, token = "ngrams", n = 1)%>%
filter(!word %in% stop_words$word)%>% # removing stop words
mutate(word = lemmatize_words(word))%>%
group_by(id)%>%
summarise(text = paste(word, collapse = " "))%>%
unnest_tokens(word, text, token = "ngrams", n = 3)
# visualing: word cloud
library(wordcloud)
tokens = textdata %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
# top words
word_count = tokens%>%
group_by(word)%>%
summarise(count = n())%>%
arrange(desc(count))%>%
slice(1:10)
ggplot(data = word_count)+
geom_bar(aes(x = word, y = count), stat = "identity",  fill = "#6699cc")+
theme_classic()
wordcloud(tokens$word, max.words = 75, colors=brewer.pal(6, "Dark2"))
View(cloud)
View(cloud)
ggplot(data = cloud)+
geom_bar(aes(x = word, y = count, fill = source), stat = "identity",  fill = "#6699cc")+
theme_classic()
ggplot(data = cloud)+
geom_bar(aes(x = word, y = count), stat = "identity",  fill = source)+
theme_classic()
ggplot(data = cloud)+
geom_bar(aes(x = word, y = count, fill = source), stat = "identity")+
theme_classic()
ggplot(data = cloud)+
geom_bar(aes(x = word, y = count, fill = source), stat = "identity", position = "dodge")+
theme_classic()
# word embeddings
library(text2vec)
View(bigrams)
View(engramsclean)
View(ngramsclean)
View(unigrams)
View(unigrams_cleaned)
# Create iterator over tokens
tokens <- space_tokenizer(unigrams_cleaned)
# Create iterator over tokens
tokens <- space_tokenizer(unigrams_cleaned$word)
View(tokens)
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5L)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
glove = GlobalVectors$new(rank = 50, x_max = 10)
shakes_wv_main = glove$fit_transform(tcm, n_iter = 100, convergence_tol = 0.001)
shakes_wv_main = glove$fit_transform(tcm, n_iter = 1600, convergence_tol = 0.001)
View(tcm)
wv_context = glove$components
# topic modelling
library(topicmodels)
data_tokenized = textdata %>%
unnest_tokens(word, content)%>%
anti_join(stop_words)%>%
count(titles,word,sort=TRUE) %>%
ungroup()
data_dtm = tokens %>%
cast_dtm(titles, word ,n)
data_tokenized = tokens %>%
unnest_tokens(word, content)%>%
anti_join(stop_words)%>%
count(titles,word,sort=TRUE) %>%
ungroup()
data = unigrams_cleaned$word
data_tokenized = data %>%
unnest_tokens(word, content)%>%
anti_join(stop_words)%>%
count(titles,word,sort=TRUE) %>%
ungroup()
data = textdata$text
data_tokenized = data %>%
unnest_tokens(word, content)%>%
anti_join(stop_words)%>%
count(titles,word,sort=TRUE) %>%
ungroup()
data_tokenized = tokens %>%
count(titles,word,sort=TRUE) %>%
ungroup()
View(tokens)
data_tokenized = tokens %>%
count(Value,word,sort=TRUE) %>%
ungroup()
library(RColorBrewer)
sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("bing"))
View(sentiment)
# word cloud
positive_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'positive') # gets sentiment score based on bing dictionary
View(positive_sentiment)
wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
negative_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'negative') # gets sentiment score based on bing dictionary
wordcloud(negative_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
library(fmsb)
library(tidyr)
nrc_sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("nrc"))%>%
filter(!is.na(sentiment))
install.packages("textdata")
library(textdata)
nrc_sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("nrc"))%>%
filter(!is.na(sentiment))
nrc_sentiment = nrc_sentiment%>%
group_by(sentiment)%>%
summarise(count = n())%>%
spread(sentiment, count)
# create rows with the min and max to be plotted
nrc_sentiment <- rbind(rep(400,10) , rep(0,10), nrc_sentiment)
radarchart(nrc_sentiment, axistype=1 ,
pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 ,
cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,20,5), cglwd=0.8,
vlcex=0.8 )
library(gutenbergr)
library(tidyr)
library(ggplot2)
words <- textdata %>%
mutate(linenumber = row_number())%>%
unnest_tokens(word, text)
sentbars = words %>%
inner_join(get_sentiments("bing"))%>%
# %/% performs integer divison, rounding down to the nearest whole number
mutate(index = linenumber %/% 25)%>%
group_by(index, sentiment)%>%
summarise(count = n())%>%
spread(sentiment, count, fill = 0) %>%
mutate(sentiment = positive - negative,
sentiment_group = ifelse(sentiment > 0, "pos", "neg"))%>%
ungroup()
ggplot(data = sentbars) +
geom_bar(aes(x = index, y = sentiment, fill = sentiment_group), stat = "identity")+
theme_classic()+
theme(
legend.position = "none",
axis.ticks.x = element_blank(),
axis.line.x = element_blank(),
axis.text.x = element_blank()
)+
scale_fill_manual(values = c("darkred", "darkgreen"))+
labs(title = "The Sentiment of Pedagogy",
x = "Change over Time (Each Bar is 25 Lines of Text)",
y = "Sentiment Score")
library(udpipe)
library(kableExtra)
library(dplyr)
## download the "english" model
udmodel <- udpipe_download_model(language = "english")
##let's load the "english' model
udmodel <- udpipe_load_model(file = udmodel$file_model)
pos <- udpipe_annotate(udmodel,
x = text)
pos <- udpipe_annotate(udmodel,
x = textdata)
pos <- udpipe_annotate(udmodel,
x = unigrams_cleaned$word)
pos <- as.data.frame(pos)
## how can get a count of words by parts of speech using the following data.
table(pos$upos)
pos <- cbind_dependencies(pos, type = "parent")
nominalsubject <- subset(pos, dep_rel %in% c("nsubj"))
nominalsubject <- nominalsubject[, c("dep_rel", "token", "token_parent")]
View(tokens)
omsi <- import(here("data", "omsidata.xlsx"))
library(papaja)
r_refs("r-references.bib")
library(tidyverse)
library(here)
library(rio)
library(tidytext)
library(dplyr)
library(tidyr)
omsi <- import(here("data", "omsidata.xlsx"))
View(omsi)
outcome_time <- c('total_time')
ID <- c('participantID')
categorical <- c('condition')
levels(omsi$condition)
omsi$condition <- as.factor(omsi$condition)
levels(omsi$condition)
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome_time, ID),
roles = c(rep('predictor',4),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(condition,
transform = function(x) x + 1,
levels=c('baseline','interrupted', 'naive', 'ped1'))
library(devtools)
library(tinytex)
library(readr)
library(magrittr)
library(recipes)
library(psych)
library(finalfit)
library(caret)
library(glmnet)
library(recipes)
library(cutpointr)
library(kableExtra)
library(pastecs)
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome_time, ID),
roles = c(rep('predictor',4),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(condition,
transform = function(x) x + 1,
levels=c('baseline','interrupted', 'naive', 'ped1'))
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome_time, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(condition,
transform = function(x) x + 1,
levels=c('baseline','interrupted', 'naive', 'ped1'))
View(blueprint)
blueprint
prepare <- prep(blueprint, training = omsidata)
prepare <- prep(blueprint, training = omsi)
library(papaja)
r_refs("r-references.bib")
library(tidyverse)
library(here)
library(rio)
library(tidytext)
library(dplyr)
library(tidyr)
library(devtools)
library(tinytex)
library(readr)
library(magrittr)
library(recipes)
library(psych)
library(finalfit)
library(caret)
library(glmnet)
library(recipes)
library(cutpointr)
library(kableExtra)
library(pastecs)
# loading datasets
mydata <- import(here("data", "pedagogy_data.xlsx"))
textdata <- import(here("data", "text_data.xlsx"))
# devdata <-
# fixing demographic variables
# pivoting longer
mydata <- mydata %>%
pivot_longer(cols = starts_with("f"),
names_to = "func",
values_to = "pedagogy")
# renaming two variables in funct column and getting rid of old func column
fun <- c(fsqueak = "squeak", flight = "light")
mydata$funct <-
as.character(fun[mydata$func])
mydata$func <- NULL
rm(fun)
# parsing words from the 'pedagogy' (text) column
tidy_words <- mydata %>%
unnest_tokens(word, pedagogy)
# removing numbers
tidy_words <- tidy_words[-grep("\\b\\d+\\b", tidy_words$word),]
# removing common/under-informative words
exclu <- tibble(word = c("the", "this", "I"))
tidy_words <- tidy_words %>%
anti_join(exclu, by = "word")
#plot
tidy_words %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
mutate(word = reorder(word, n)) %>% # make y-axis ordered by n
slice(1:15) %>% # select only the first 15 rows
ggplot(aes(n, word)) +
geom_col(fill = "royalblue", alpha = .7) +
scale_x_continuous(expand = c(0,0)) +
theme_minimal() +
theme(
panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.x = element_line(color = "gray80")
) +
labs(
x = "Word Frequency",
y = "Word",
title = "Top 15 most frequently occurring words across all pedagogy types",
)
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
# tokenizing
unigrams = textdata %>% unnest_tokens(word, text, token = "ngrams", n = 1)
bigrams = textdata %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)
# no stop words
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)
# stemming
library(SnowballC)
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)%>%
mutate(word = wordStem(word))
# lemmatization
library(textstem)
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)%>%
mutate(word = lemmatize_words(word))
# cleaning n-grams
library(tm)
ngramsclean <- textdata %>%
unnest_tokens(word, text, token = "ngrams", n = 1)%>%
filter(!word %in% stop_words$word)%>% # removing stop words
mutate(word = lemmatize_words(word))%>%
group_by(id)%>%
summarise(text = paste(word, collapse = " "))%>%
unnest_tokens(word, text, token = "ngrams", n = 3)
# visualing: word cloud
library(wordcloud)
tokens = textdata %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
# top words
word_count = tokens%>%
group_by(word)%>%
summarise(count = n())%>%
arrange(desc(count))%>%
slice(1:10)
# word cloud--zoom in
cloud <- tokens %>%
group_by(source, word) %>%
summarise(count = n())%>%
arrange(desc(count))%>%
slice(1:10)
ggplot(data = cloud)+
geom_bar(aes(x = word, y = count, fill = source), stat = "identity", position = "dodge")+
theme_classic()
wordcloud(tokens$word, max.words = 75, colors=brewer.pal(6, "Dark2"))
# word embeddings
library(text2vec)
# text8_file = "~/text8"
# unzip ("text8.zip", files = "text8", exdir = "~/")
# wiki = readLines(text8_file, n = 1, warn = FALSE)
# Create iterator over tokens
tokens <- space_tokenizer(unigrams_cleaned$word)
# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5L)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
glove = GlobalVectors$new(rank = 50, x_max = 10)
# shakes_wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.001)
# wv_context = glove$components
# word_vectors = shakes_wv_main  + t(wv_context)
# berlin = word_vectors["paris", , drop = FALSE] -
#  word_vectors["france", , drop = FALSE] +
#  word_vectors["germany", , drop = FALSE]
# berlin_cos_sim = sim2(x = word_vectors, y = berlin, method = "cosine", norm = "l2")
# head(sort(berlin_cos_sim[,1], decreasing = TRUE), 5)
# topic modelling--more work to do
library(topicmodels)
# sentiment analysis
library(RColorBrewer)
sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("bing"))
# word cloud
positive_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'positive')
wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
negative_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'negative')
wordcloud(negative_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
#### sentiment radar chart
library(fmsb)
library(tidyr)
library(textdata)
nrc_sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("nrc"))%>%
filter(!is.na(sentiment))
nrc_sentiment = nrc_sentiment%>%
group_by(sentiment)%>%
summarise(count = n())%>%
spread(sentiment, count)
# create rows with the min and max to be plotted
nrc_sentiment <- rbind(rep(400,10) , rep(0,10), nrc_sentiment)
radarchart(nrc_sentiment, axistype=1 ,
pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 ,
cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,20,5), cglwd=0.8,
vlcex=0.8 )
#### sentiment over time
library(gutenbergr)
library(tidyr)
library(ggplot2)
words <- textdata %>%
mutate(linenumber = row_number())%>%
unnest_tokens(word, text)
sentbars = words %>%
inner_join(get_sentiments("bing"))%>%
# %/% performs integer divison, rounding down to the nearest whole number
mutate(index = linenumber %/% 25)%>%
group_by(index, sentiment)%>%
summarise(count = n())%>%
spread(sentiment, count, fill = 0) %>%
mutate(sentiment = positive - negative,
sentiment_group = ifelse(sentiment > 0, "pos", "neg"))%>%
ungroup()
# the plot itself
ggplot(data = sentbars) +
geom_bar(aes(x = index, y = sentiment, fill = sentiment_group), stat = "identity")+
theme_classic()+
theme(
legend.position = "none",
axis.ticks.x = element_blank(),
axis.line.x = element_blank(),
axis.text.x = element_blank()
)+
scale_fill_manual(values = c("darkred", "darkgreen"))+
labs(title = "The Sentiment of Pedagogy",
x = "Change over Time (Each Bar is 25 Lines of Text)",
y = "Sentiment Score")
# parts of speech tagging
## load udpipe
library(udpipe)
library(kableExtra)
## download the "english" model
udmodel <- udpipe_download_model(language = "english")
##let's load the "english' model
udmodel <- udpipe_load_model(file = udmodel$file_model)
pos <- udpipe_annotate(udmodel,
x = unigrams_cleaned$word)
pos <- as.data.frame(pos)
## how can get a count of words by parts of speech using the following data.
table(pos$upos)
pos <- cbind_dependencies(pos, type = "parent")
nominalsubject <- subset(pos, dep_rel %in% c("nsubj"))
nominalsubject <- nominalsubject[, c("dep_rel", "token", "token_parent")]
omsi <- import(here("data", "omsidata.xlsx"))
omsi$condition <- as.factor(omsi$condition)
outcome_time <- c('total_time')
ID <- c('participantID')
categorical <- c('condition')
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome_time, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(condition,
transform = function(x) x + 1,
levels=c('baseline','interrupted', 'naive', 'ped1'))
blueprint
prepare <- prep(blueprint, training = omsi)
prepare_set_jee
baked <- bake(prepare_set_jee, new_data = set_data)
# preparing blueprint for AT
blueprint_set_at <- recipe(x  = set_data,
vars  = c(categorical, outcome_at, ID),
roles = c(rep('predictor',129),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(AT,
transform = function(x) x + 1,
levels=c('High','Low'))
blueprint_set_at
prepare_set_at <- prep(blueprint_set_jee, training = set_data)
prepare_set_at
baked_set_at <- bake(prepare_set_at, new_data = set_data)
wordcloud_overall <- wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
wordcloud_positive <- wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
wordcloud_positive(x, main = "wordcloud for positive sentiment")
wordcloud_positive <- wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"), par(mfrow = "Wordcloud for positive sentiment"))
sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("bing"))
# word cloud
positive_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'positive')
wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
negative_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'negative')
wordcloud(negative_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
