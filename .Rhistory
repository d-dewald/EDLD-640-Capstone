summarise(count = n())%>%
spread(sentiment, count, fill = 0) %>%
mutate(sentiment = positive - negative,
sentiment_group = ifelse(sentiment > 0, "pos", "neg"))%>%
ungroup()
# the plot itself
ggplot(data = sentbars) +
geom_bar(aes(x = index, y = sentiment, fill = sentiment_group), stat = "identity")+
theme_classic()+
theme(
legend.position = "none",
axis.ticks.x = element_blank(),
axis.line.x = element_blank(),
axis.text.x = element_blank()
)+
scale_fill_manual(values = c("darkred", "darkgreen"))+
labs(title = "The Sentiment of Pedagogy",
x = "Change over Time (Each Bar is 25 Lines of Text)",
y = "Sentiment Score")
# parts of speech tagging
## load udpipe
library(udpipe)
library(kableExtra)
## download the "english" model
udmodel <- udpipe_download_model(language = "english")
##let's load the "english' model
udmodel <- udpipe_load_model(file = udmodel$file_model)
pos <- udpipe_annotate(udmodel,
x = unigrams_cleaned$word)
pos <- as.data.frame(pos)
## how can get a count of words by parts of speech using the following data.
table(pos$upos)
pos <- cbind_dependencies(pos, type = "parent")
nominalsubject <- subset(pos, dep_rel %in% c("nsubj"))
nominalsubject <- nominalsubject[, c("dep_rel", "token", "token_parent")]
rm(list=ls())
library(papaja)
r_refs("r-references.bib")
library(tidyverse)
library(here)
library(rio)
library(tidytext)
library(dplyr)
library(tidyr)
library(devtools)
library(tinytex)
library(readr)
library(magrittr)
library(recipes)
library(psych)
library(finalfit)
library(caret)
library(glmnet)
library(recipes)
library(cutpointr)
library(kableExtra)
library(pastecs)
# loading datasets
mydata <- import(here("data", "pedagogy_data.xlsx"))
textdata <- import(here("data", "text_data.xlsx"))
# devdata <-
# fixing demographic variables
# pivoting longer
mydata <- mydata %>%
pivot_longer(cols = starts_with("f"),
names_to = "func",
values_to = "pedagogy")
# renaming two variables in funct column and getting rid of old func column
fun <- c(fsqueak = "squeak", flight = "light")
mydata$funct <-
as.character(fun[mydata$func])
mydata$func <- NULL
rm(fun)
# parsing words from the 'pedagogy' (text) column
tidy_words <- mydata %>%
unnest_tokens(word, pedagogy)
# removing numbers
tidy_words <- tidy_words[-grep("\\b\\d+\\b", tidy_words$word),]
# removing common/under-informative words
exclu <- tibble(word = c("the", "this", "I"))
tidy_words <- tidy_words %>%
anti_join(exclu, by = "word")
#plot
tidy_words %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
mutate(word = reorder(word, n)) %>% # make y-axis ordered by n
slice(1:15) %>% # select only the first 15 rows
ggplot(aes(n, word)) +
geom_col(fill = "royalblue", alpha = .7) +
scale_x_continuous(expand = c(0,0)) +
theme_minimal() +
theme(
panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.x = element_line(color = "gray80")
) +
labs(
x = "Word Frequency",
y = "Word",
title = "Top 15 most frequently occurring words across all pedagogy types",
)
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
# tokenizing
unigrams = textdata %>% unnest_tokens(word, text, token = "ngrams", n = 1)
bigrams = textdata %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)
# no stop words
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)
# stemming
library(SnowballC)
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)%>%
mutate(word = wordStem(word))
# lemmatization
library(textstem)
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)%>%
mutate(word = lemmatize_words(word))
# cleaning n-grams
library(tm)
ngramsclean <- textdata %>%
unnest_tokens(word, text, token = "ngrams", n = 1)%>%
filter(!word %in% stop_words$word)%>% # removing stop words
mutate(word = lemmatize_words(word))%>%
group_by(id)%>%
summarise(text = paste(word, collapse = " "))%>%
unnest_tokens(word, text, token = "ngrams", n = 3)
# visualing: word cloud
library(wordcloud)
tokens = textdata %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
# top words
word_count = tokens%>%
group_by(word)%>%
summarise(count = n())%>%
arrange(desc(count))%>%
slice(1:10)
# word cloud--zoom in
cloud <- tokens %>%
group_by(source, word) %>%
summarise(count = n())%>%
arrange(desc(count))%>%
slice(1:10)
wordcloud(tokens$word, max.words = 75, colors=brewer.pal(6, "Dark2"))
# word embeddings
library(text2vec)
# text8_file = "~/text8"
# unzip ("text8.zip", files = "text8", exdir = "~/")
# wiki = readLines(text8_file, n = 1, warn = FALSE)
# Create iterator over tokens
tokens <- space_tokenizer(unigrams_cleaned$word)
# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5L)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
glove = GlobalVectors$new(rank = 50, x_max = 10)
# shakes_wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.001)
# wv_context = glove$components
# word_vectors = shakes_wv_main  + t(wv_context)
# berlin = word_vectors["paris", , drop = FALSE] -
#  word_vectors["france", , drop = FALSE] +
#  word_vectors["germany", , drop = FALSE]
# berlin_cos_sim = sim2(x = word_vectors, y = berlin, method = "cosine", norm = "l2")
# head(sort(berlin_cos_sim[,1], decreasing = TRUE), 5)
# topic modelling--more work to do
library(topicmodels)
# sentiment analysis
library(RColorBrewer)
sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("bing"))
# word cloud
positive_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'positive')
wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
negative_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'negative')
wordcloud(negative_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
#### sentiment radar chart
library(fmsb)
library(tidyr)
library(textdata)
nrc_sentiment = textdata %>% #this allows us to retain the row number
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("nrc"))%>%
filter(!is.na(sentiment))
nrc_sentiment = nrc_sentiment%>%
group_by(sentiment)%>%
summarise(count = n())%>%
spread(sentiment, count)
# create rows with the min and max to be plotted
nrc_sentiment <- rbind(rep(400,10) , rep(0,10), nrc_sentiment)
radarchart(nrc_sentiment, axistype=1 ,
pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 ,
cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,20,5), cglwd=0.8,
vlcex=0.8 )
#### sentiment over time
library(gutenbergr)
library(tidyr)
library(ggplot2)
words <- textdata %>%
mutate(linenumber = row_number())%>%
unnest_tokens(word, text)
sentbars = words %>%
inner_join(get_sentiments("bing"))%>%
# %/% performs integer divison, rounding down to the nearest whole number
mutate(index = linenumber %/% 25)%>%
group_by(index, sentiment)%>%
summarise(count = n())%>%
spread(sentiment, count, fill = 0) %>%
mutate(sentiment = positive - negative,
sentiment_group = ifelse(sentiment > 0, "pos", "neg"))%>%
ungroup()
# the plot itself
ggplot(data = sentbars) +
geom_bar(aes(x = index, y = sentiment, fill = sentiment_group), stat = "identity")+
theme_classic()+
theme(
legend.position = "none",
axis.ticks.x = element_blank(),
axis.line.x = element_blank(),
axis.text.x = element_blank()
)+
scale_fill_manual(values = c("darkred", "darkgreen"))+
labs(title = "The Sentiment of Pedagogy",
x = "Change over Time (Each Bar is 25 Lines of Text)",
y = "Sentiment Score")
# parts of speech tagging
## load udpipe
library(udpipe)
library(kableExtra)
## download the "english" model
udmodel <- udpipe_download_model(language = "english")
##let's load the "english' model
udmodel <- udpipe_load_model(file = udmodel$file_model)
pos <- udpipe_annotate(udmodel,
x = unigrams_cleaned$word)
pos <- as.data.frame(pos)
## how can get a count of words by parts of speech using the following data.
table(pos$upos)
pos <- cbind_dependencies(pos, type = "parent")
nominalsubject <- subset(pos, dep_rel %in% c("nsubj"))
nominalsubject <- nominalsubject[, c("dep_rel", "token", "token_parent")]
ggplot(data = cloud)+
geom_bar(aes(x = word, y = count, fill = source), stat = "identity", position = "dodge")+
theme_classic()
omsi <- import(here("data", "omsidata.xlsx"))
View(omsi)
View(omsi)
omsi$condition <- as.factor(omsi$condition)
omsi <- import(here("data", "omsidata.xlsx"))
omsi$condition <- as.factor(omsi$condition)
outcome <- c('squeaker_discovered')
ID <- c('participantID')
categorical <- c('condition')
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c('Y' 'N'))
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c('Y', 'N'))
set.seed(11102021)  # for reproducibility
ped     <- sample(1:nrow(omsi), round(nrow(omsi) * 0.8))
ped_train  <- omsi[ped], ]
ped_train  <- omsi[ped, ]
ped_test  <- omsi[-ped, ]
set.seed(11152021) # for reproducibility
omsi_tr = ped_train[sample(nrow(ped_train)),]
folds = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
for(i in 1:10){
omsi.indices[[i]] <- which(folds!=i)
}
for(i in 1:10){
omsi.indices[[i]] <- which(folds!=i)
}
for(i in 1:10){
omsi.indices[[i]] <- which(folds!=i)
}
for(i in 1:10){
omsi_indices[[i]] <- which(folds!=i)
}
folds_omsi = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
for(i in 1:10){
omsi_indices[[i]] <- which(folds_omsi!=i)
}
for(i in 1:10){
omsi_indices[[i]] <- which(folds_omsi!=i)
}
class(omsi_indices)
# Creating the list for each fold
omsi_indices <- vector(mode = 'list',length = 10)
class(omsi_indices)
for(i in 1:10){
omsi_indices[[i]] <- new[i]
i <- i + 1
}
omsi_indices <- vector(mode = 'list',length = 10)
for(i in 1:10){
omsi_indices[[i]] <- new[i]
i <- i + 1
}
set.seed(125)
cv <- trainControl(method    = "cv",
number = 10)
ped_mod <- caret::train(blueprint,
data      = omsi_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
cv <- trainControl(method    = "cv",
classProbs      = TRUE,
summaryFunction = mnLogLoss)
number = 10)
set.seed(125)
cv <- trainControl(method    = "cv",
classProbs      = TRUE,
summaryFunction = mnLogLoss,
number = 10)
ped_mod <- caret::train(blueprint,
data      = omsi_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
omsi <- import(here("data", "omsidata.xlsx"))
omsi$squeaker_discovered <- as.numeric(omsi$squeaker_discovered)
omsi <- import(here("data", "omsidata.xlsx"))
omsi$condition <- as.factor(omsi$condition)
outcome <- c('squeaker_discovered')
ID <- c('participantID')
categorical <- c('condition')
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c(1, 0))
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c('1', '0'))
blueprint
set.seed(11102021)  # for reproducibility
ped     <- sample(1:nrow(omsi), round(nrow(omsi) * 0.8))
ped_train  <- omsi[ped, ]
ped_test  <- omsi[-ped, ]
set.seed(11152021) # for reproducibility
omsi_tr = ped_train[sample(nrow(ped_train)),]
# Creating 10 folds with equal size
folds_omsi = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
set.seed(125)
cv <- trainControl(method    = "cv",
classProbs      = TRUE,
summaryFunction = mnLogLoss,
number = 10)
ped_mod <- caret::train(blueprint,
data      = omsi_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
View(omsi_indices)
View(omsi_tr)
omsi <- import(here("data", "omsidata.xlsx"))
omsi$condition <- as.factor(omsi$condition)
outcome <- c('squeaker_discovered')
ID <- c('participantID')
categorical <- c('condition')
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c('Y', 'N'))
set.seed(11102021)  # for reproducibility
ped     <- sample(1:nrow(omsi), round(nrow(omsi) * 0.8))
ped_train  <- omsi[ped, ]
ped_test  <- omsi[-ped, ]
set.seed(11152021) # for reproducibility
omsi_tr = ped_train[sample(nrow(ped_train)),]
folds_omsi = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
set.seed(125)
cv <- trainControl(method    = "cv",
classProbs      = TRUE,
summaryFunction = mnLogLoss,
number = 10)
ped_mod <- caret::train(blueprint,
data      = omsi_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(condition,
transform = function(x) x + 1,
levels=c('ped1', 'baseline', 'naive', 'interrupted'))
set.seed(11102021)  # for reproducibility
ped     <- sample(1:nrow(omsi), round(nrow(omsi) * 0.8))
ped_train  <- omsi[ped, ]
ped_test  <- omsi[-ped, ]
omsi_tr = ped_train[sample(nrow(ped_train)),]
folds_omsi = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
set.seed(125)
cv <- trainControl(method    = "cv",
classProbs      = TRUE,
summaryFunction = mnLogLoss,
number = 10)
ped_mod <- caret::train(blueprint,
data      = omsi_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c('Y', 'N')) %>%
step_num2factor(condition,
transform = function(x) x + 1,
levels=c('ped1', 'baseline', 'interrupted', 'naive'))
set.seed(11102021)  # for reproducibility
ped     <- sample(1:nrow(omsi), round(nrow(omsi) * 0.8))
ped_train  <- omsi[ped, ]
ped_test  <- omsi[-ped, ]
set.seed(11152021) # for reproducibility
omsi_tr = ped_train[sample(nrow(ped_train)),]
# Creating 10 folds with equal size
folds_omsi = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
set.seed(125)
cv <- trainControl(method    = "cv",
classProbs      = TRUE,
summaryFunction = mnLogLoss,
number = 10)
ped_mod <- caret::train(blueprint,
data      = omsi_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
omsi <- import(here("data", "omsidata.xlsx"))
omsi$condition <- as.factor(omsi$condition)
outcome <- c('squeaker_discovered')
ID <- c('participant')
categorical <- c('condition')
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c('Y', 'N')) %>%
step_num2factor(condition,
transform = function(x) x + 1,
levels=c('ped1', 'baseline', 'interrupted', 'naive'))
set.seed(11102021)  # for reproducibility
ped     <- sample(1:nrow(omsi), round(nrow(omsi) * 0.8))
ped_train  <- omsi[ped, ]
ped_test  <- omsi[-ped, ]
set.seed(11152021) # for reproducibility
omsi_tr = ped_train[sample(nrow(ped_train)),]
# Creating 10 folds with equal size
folds_omsi = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
set.seed(125)
cv <- trainControl(method    = "cv",
classProbs      = TRUE,
summaryFunction = mnLogLoss,
number = 10)
ped_mod <- caret::train(blueprint,
data      = omsi_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
View(cv)
cv$method <- as.numberic(cv$method)
cv$method <- as.numeric(cv$method)
