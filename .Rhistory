positive_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'positive')
wordcloud(positive_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
negative_sentiment = sentiment%>% filter(!is.na(sentiment),
sentiment == 'negative')
wordcloud(negative_sentiment$word, random.order = FALSE, colors=brewer.pal(6, "Dark2"))
#### sentiment radar chart
library(fmsb)
library(tidyr)
library(textdata)
nrc_sentiment = textdata %>% #this allows us to retain the row number
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("nrc"))%>%
filter(!is.na(sentiment))
nrc_sentiment = nrc_sentiment%>%
group_by(sentiment)%>%
summarise(count = n())%>%
spread(sentiment, count)
# create rows with the min and max to be plotted
nrc_sentiment <- rbind(rep(400,10) , rep(0,10), nrc_sentiment)
radarchart(nrc_sentiment, axistype=1 ,
pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 ,
cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,20,5), cglwd=0.8,
vlcex=0.8 )
#### sentiment over time
library(gutenbergr)
library(tidyr)
library(ggplot2)
words <- textdata %>%
mutate(linenumber = row_number())%>%
unnest_tokens(word, text)
sentbars = words %>%
inner_join(get_sentiments("bing"))%>%
# %/% performs integer divison, rounding down to the nearest whole number
mutate(index = linenumber %/% 25)%>%
group_by(index, sentiment)%>%
summarise(count = n())%>%
spread(sentiment, count, fill = 0) %>%
mutate(sentiment = positive - negative,
sentiment_group = ifelse(sentiment > 0, "pos", "neg"))%>%
ungroup()
# the plot itself
ggplot(data = sentbars) +
geom_bar(aes(x = index, y = sentiment, fill = sentiment_group), stat = "identity")+
theme_classic()+
theme(
legend.position = "none",
axis.ticks.x = element_blank(),
axis.line.x = element_blank(),
axis.text.x = element_blank()
)+
scale_fill_manual(values = c("darkred", "darkgreen"))+
labs(title = "The Sentiment of Pedagogy",
x = "Change over Time (Each Bar is 25 Lines of Text)",
y = "Sentiment Score")
# parts of speech tagging
## load udpipe
library(udpipe)
library(kableExtra)
## download the "english" model
udmodel <- udpipe_download_model(language = "english")
##let's load the "english' model
udmodel <- udpipe_load_model(file = udmodel$file_model)
pos <- udpipe_annotate(udmodel,
x = unigrams_cleaned$word)
pos <- as.data.frame(pos)
## how can get a count of words by parts of speech using the following data.
table(pos$upos)
pos <- cbind_dependencies(pos, type = "parent")
nominalsubject <- subset(pos, dep_rel %in% c("nsubj"))
nominalsubject <- nominalsubject[, c("dep_rel", "token", "token_parent")]
omsi <- import(here("data", "omsidata.xlsx"))
omsi$condition <- as.factor(omsi$condition)
outcome <- c('squeaker_discovered')
ID <- c('participant')
categorical <- c('condition')
# Problem: currently getting an error when I try to train the model.The error is coming from step_medianimpute(). step_medianimpute() requires all the variables to be numeric but it is being passed factor variables with all_predictors().One way to fix this problem is by rearranging your recipe to create dummy variables before you impute.need to dummy code
blueprint <- recipe(x  = omsi,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_indicate_na(all_of(categorical)) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c('Y', 'N')) %>%
step_num2factor(condition,
transform = function(x) x + 1,
levels=c('ped1', 'baseline', 'interrupted', 'naive'))
blueprint
# let's split into testing and training sets
# Let the training data have the 80% of cases and the test data have the 20% of the cases.
set.seed(11102021)  # for reproducibility
ped     <- sample(1:nrow(omsi), round(nrow(omsi) * 0.8))
ped_train  <- omsi[ped, ]
ped_test  <- omsi[-ped, ]
# 10 fold cross validation
set.seed(11152021) # for reproducibility
omsi_tr = ped_train[sample(nrow(ped_train)),]
# Creating 10 folds with equal size
folds_omsi = cut(seq(1,nrow(omsi_tr)),breaks=10,labels=FALSE)
set.seed(125)
cv <- trainControl(method    = "cv",
classProbs      = TRUE,
summaryFunction = mnLogLoss,
number = 10)
ped_mod <- caret::train(blueprint,
data      = omsi_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
rlang::last_error()
View(omsi)
View(nrc_sentiment)
library(papaja)
r_refs("r-references.bib")
library(tidyverse)
library(here)
library(rio)
library(tidytext)
library(dplyr)
library(tidyr)
library(devtools)
library(tinytex)
library(readr)
library(magrittr)
library(recipes)
library(psych)
library(finalfit)
library(caret)
library(glmnet)
library(recipes)
library(cutpointr)
library(kableExtra)
library(pastecs)
mydata <- import(here("data", "pedagogy_data.xlsx"))
textdata <- import(here("data", "text_data.xlsx"))
# devdata <-
# fixing demographic variables
# pivoting longer
mydata <- mydata %>%
pivot_longer(cols = starts_with("f"),
names_to = "func",
values_to = "pedagogy")
# renaming two variables in funct column and getting rid of old func column
fun <- c(fsqueak = "squeak", flight = "light")
mydata$funct <-
as.character(fun[mydata$func])
mydata$func <- NULL
rm(fun)
View(textdata)
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
# tokenizing
unigrams = textdata %>% unnest_tokens(word, text, token = "ngrams", n = 1)
bigrams = textdata %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)
# no stop words
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)
# stemming
library(SnowballC)
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)%>%
mutate(word = wordStem(word))
# lemmatization
library(textstem)
unigrams_cleaned = unigrams %>%
filter(!word %in% stop_words$word)%>%
mutate(word = lemmatize_words(word))
library(tm)
ngramsclean <- textdata %>%
unnest_tokens(word, text, token = "ngrams", n = 1)%>%
filter(!word %in% stop_words$word)%>% # removing stop words
mutate(word = lemmatize_words(word))%>%
group_by(id)%>%
summarise(text = paste(word, collapse = " "))%>%
unnest_tokens(word, text, token = "ngrams", n = 3)
tokens = textdata %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
View(tokens)
sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("bing"))
View(sentiment)
library(writexl)
write_xlsx(sentiment, "sentiment.xlsx")
library(readxl)
sentimentdata <- read_excel("C:\Users\ddewa\OneDrive\Documents\Oregon\Courses\datascience spec\edld_640\EDLD-640-Capstone")
sentimentdata <- read_excel("C:/Users/ddewa/OneDrive/Documents/Oregon/Courses/datascience spec/edld_640/EDLD-640-Capstone")
sentimentdata <- import(here("EDLD-640-Capstone", "sentiment.xlsx"))
sentimentdata <- import(here("data", "sentiment.xlsx"))
View(sentimentdata)
sentiment_by_condition <- sentimentdata %>%
group_by(condition) %>%
count(sentiment)
View(sentiment_by_condition)
sentimentdata <- import(here("data", "sentiment.xlsx"))
sentiment_by_condition <- sentimentdata %>%
group_by(condition) %>%
count(sentiment)
View(sentiment_by_condition)
View(textdata)
View(tokens)
View(unigrams)
View(unigrams_cleaned)
View(tokens)
View(tidy_words)
View(word_count)
View(textdata)
View(ngramsclean)
View(tidy_words)
View(tokens)
View(tokens)
word_countID = tokens%>%
group_by(id) %>%
summarise(count = n())
View(word_countID)
View(sentimentdata)
sentimentdata <- left_join(sentimentdata, word_counID, by = "id")
sentimentdata <- left_join(sentimentdata, word_countID, by = "id")
View(sentimentdata)
View(sentimentdata)
View(textdata)
View(mydata)
sentimentdata <- left_join(sentimentdata, mydata, by = "id")
View(sentimentdata)
sentimentdata <- left_join(sentimentdata, word_countID, by = "id")
sentiment = textdata %>% #this allows us to retain the row number/the tweet
unnest_tokens(word, text)%>% # this unnests the tweets into words
anti_join(stop_words)%>% #removes common words (stop words)
left_join(get_sentiments("bing"))
library(writexl)
write_xlsx(sentiment, "sentiment.xlsx")
sentimentdata <- import(here("data", "sentiment.xlsx"))
sentimentdata <- import(here("data", "sentiment.xlsx"))
sentimentdata <- left_join(sentimentdata, word_countID, by = "id")
View(sentimentdata)
outcome <- c('sentiment_numeric')
ID <- c('id')
categorical <- c('condition', 'source', 'function', 'count')
blueprint <- recipe(x  = sentiment,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_zv(all_of(categorical)) %>%
step_num2factor(outcome,
transform = function(x) x + 1,
levels=c('Negative', 'Positive'))
outcome <- c('sentiment')
blueprint <- recipe(x  = sentiment,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_zv(all_of(categorical))
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'),'outcome','ID')) %>%
step_zv(all_of(categorical))
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome, ID),
roles = c('outcome',rep('predictor',4))) %>%
step_zv(all_of(categorical))
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome, ID),
roles = c('outcome',rep('predictor',5))) %>%
step_zv(all_of(categorical))
blueprint
set.seed(11102021)  # for reproducibility
sen      <- sample(1:nrow(sentimentdata), round(nrow(sentimentdata) * 0.8))
sen_train  <- sentimentdata[sen, ]
sen_test  <- sentimentdata[-sen, ]
set.seed(11152021) # for reproducibility
sen_tr = sen_train[sample(nrow(sen_train)),]
folds = cut(seq(1,nrow(sen_tr)),breaks=10,labels=FALSE)
for(i in 1:10){
sen.indices[[i]] <- which(folds!=i)
}
# Creating the list for each fold
sen.indices <- vector('list',10)
for(i in 1:10){
sen.indices[[i]] <- which(folds!=i)
}
cv <- trainControl(method    = "cv",
index           = sen.indices,
classProbs      = TRUE,
summaryFunction = mnLogLoss)
caret_mod <- caret::train(blueprint,
data      = sen_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
caret_mod
predicted_test <- predict(caret_mod, sen_test, type='prob')
head(predicted_test)
caret::getModelInfo()$glmnet$parameters
grid_ridge <- data.frame(alpha = 0, lambda = seq(0.01,2,.01))
ridge <- caret::train(blueprint,
data      = sen_train,
method    = "glmnet",
family = 'binomial',
metric = 'logLoss',
trControl = cv,
tuneGrid  = grid_ridge)
grid_lasso <- data.frame(alpha = 1, lambda = seq(0.01,3,.01))
lasso <- caret::train(blueprint,
data      = ne_train,
method    = "glmnet",
trControl = cv,
tuneGrid  = grid_lasso)
lasso <- caret::train(blueprint,
data      = sen_train,
method    = "glmnet",
trControl = cv,
tuneGrid  = grid_lasso)
predicted_test <- predict(caret_mod, tweet_test, type='prob')
predicted_test <- predict(caret_mod, sen_test, type='prob')
dim(predicted_test)
pred_class <- ifelse(predicted_test$Positive>.5,1,0)
confusion <- table(sen_test$sentiment,pred_class)
cut.obj <- cutpointr(x     = predicted_test$Positive,
class = sen_test$sentiment)
predicted_test <- predict(caret_mod, sen_test, type='prob')
View(predicted_test)
predicted_test <- predicted_test(na.rm = TRUE)
predicted_test[complete.cases(predicted_test), ]
predicted_test <- predicted_test[complete.cases(predicted_test), ]
dim(predicted_test)
pred_class <- ifelse(predicted_test$Positive>.5,1,0)
confusion <- table(sen_test$sentiment,pred_class)
View(sen_test)
cut.obj <- cutpointr(x     = predicted_test$Positive,
class = sen_test$sentiment)
View(predicted_test)
mean(predicted_test$constrain)
mean(predicted_test$enhance)
View(caret_mod)
View(predicted_test)
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome),
roles = c('outcome',rep('predictor',5))) %>%
step_zv(all_of(categorical))
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome),
roles = c('outcome',rep('predictor',4))) %>%
step_zv(all_of(categorical))
View(blueprint)
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome),
roles = c(rep('predictor',4)), 'outcome') %>%
step_zv(all_of(categorical))
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor')), 'outcome', 'ID') %>%
step_zv(all_of(categorical))
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor'), 'outcome', 'ID')) %>%
step_zv(all_of(categorical))
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome, ID),
roles = c('predictor', 'outcome', 'ID')) %>%
step_zv(all_of(categorical))
outcome <- c('sentiment')
ID <- c('id')
categorical <- c('condition', 'source', 'function', 'count')
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome, ID),
roles = c('predictor', 'outcome', 'ID'))
blueprint <- recipe(x  = sentimentdata,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor',4), 'outcome', 'ID')) %>%
step_zv(all_of(categorical))
blueprint
set.seed(11102021)  # for reproducibility
sen      <- sample(1:nrow(sentimentdata), round(nrow(sentimentdata) * 0.8))
sen_train  <- sentimentdata[sen, ]
sen_test  <- sentimentdata[-sen, ]
set.seed(11152021) # for reproducibility
sen_tr = sen_train[sample(nrow(sen_train)),]
folds = cut(seq(1,nrow(sen_tr)),breaks=10,labels=FALSE)
# Creating the list for each fold
sen.indices <- vector('list',10)
for(i in 1:10){
sen.indices[[i]] <- which(folds!=i)
}
cv <- trainControl(method    = "cv",
index           = sen.indices,
classProbs      = TRUE,
summaryFunction = mnLogLoss)
caret_mod <- caret::train(blueprint,
data      = sen_tr,
method    = "glm",
family    = 'binomial',
metric    = 'logLoss',
trControl = cv)
caret_mod
predicted_test <- predict(caret_mod, sen_test, type='prob')
head(predicted_test)
grid_ridge <- data.frame(alpha = 0, lambda = seq(0.01,2,.01))
ridge <- caret::train(blueprint,
data      = sen_train,
method    = "glmnet",
family = 'binomial',
metric = 'logLoss',
trControl = cv,
tuneGrid  = grid_ridge)
# Evaluate the model on the training dataset
predicted_tr <- predict(caret_mod, sen_train)
rsq_tr <- cor(sen_train$sentiment,sen_tr)^2
rsq_tr <- cor(sen_train$sentiment_numeric,sen_tr)^2
View(sen_tr)
# Evaluate the model on the test dataset
predicted_tr <- predict(caret_mod, sen_train)
rsq_te <- cor(sentimentdata$sentiment,predicted_te)^2
# Evaluate the model on the test dataset
predicted_tr <- predict(caret_mod, sen_test)
# Evaluate the model on the test dataset
predicted_te <- predict(caret_mod, sen_test)
rsq_te <- cor(sentimentdata$sentiment,predicted_te)^2
predicted_te <- as.numeric(predicted_te)
rsq_te <- cor(sentimentdata$sentiment_numeric,predicted_te)^2
rsq_te <- cor(sen_test$sentiment_numeric,predicted_te)^2
rsq_te
View(sen_test)
predicted_eval <- as.dataframe(predicted_te, sen_test$sentiment_numeric)
predicted_eval <- as.data.frame(predicted_te, sen_test$sentiment_numeric)
View(predicted_eval)
sen_test_numeric <- sen_test$sentiment_numeric
predicted_eval <- as.data.frame(predicted_te, sen_test_numeric)
sen_test_numeric <- c(sen_test$sentiment_numeric)
predicted_eval <- as.data.frame(predicted_te, sen_test_numeric)
predicted_te <- as.numeric(predicted_te)
predicted_eval <- as.data.frame(predicted_te, sen_test_numeric)
predicted_eval <- data.frame(predicted_te, sen_test_numeric)
View(predicted_eval)
rsq_te <- cor(predicted_eval$predicted_te,predicted_eval$sen_test_numeric)^2
rsq_te
predicted_eval <- predicted_eval[complete.cases(predicted_eval), ]
rsq_te <- cor(predicted_eval$predicted_te,predicted_eval$sen_test_numeric)^2
rsq_te
View(sen_test)
sen_test_numeric <- sen_test$sentiment_numeric
predicted_eval <- data.frame(predicted_te, sen_test_numeric)
predicted_eval <- predicted_eval[complete.cases(predicted_eval), ]
rsq_te <- cor(predicted_eval$predicted_te,predicted_eval$sen_test_numeric)^2
rsq_te
View(word_countID)
View(sentiment_by_condition)
text_dummy <- read_excel("data", "text_data_dummy.xlsx")
textdummy <- import(here("data", "text_data_dummy.xlsx"))
View(textdummy)
set.seed(10152022) # for reproducibility
ex
loc <- sample(1:nrow(textdummy), round(nrow(textdummy) * 0.9))
rank_tr <- textdummy[loc, ]
rank_te <- textdummy[-loc, ]
#randomly shuffle training data
rank_tr = rank_tr[sample(nrow(rank_tr))]
#randomly shuffle training data
rank_tr = rank_tr[sample(nrow(rank_tr)),]
folds = cut(seq(1,nrow(rank_tr)),breaks=10,labels=FALSE)
my.indices <- vector('list',10)
for(i in 1:10){
my.indices[[i]] <- which(folds!=i)
}
cv <- trainControl(method = "cv",
index  = my.indices)
require(caret)
require(kknn)
install.package('kknn')
install.packages('kknn')
require(kknn)
getModelInfo()$kknn$parameters
grid <- expand.grid(kmax    = 3:25,
distance = c(1,2,3),
kernel   = c('epanechnikov','rectangular'))
grid
require(doParallel)
ncores <- 8    # depends on the number of cores available in your computer
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)
View(textdummy)
outcome <- c('rank')
ID <- c('id')
categorical <- c('condition', 'source', 'function')
blueprint <- recipe(x  = textdummy,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor',3), 'outcome', 'ID')) %>%
step_zv(all_of(categorical))
blueprint_textdummy <- recipe(x  = textdummy,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor',3), 'outcome', 'ID')) %>%
step_zv(all_of(categorical))
caret_knn_readability <- caret::train(blueprint_textdummy,
data      = rank_tr,
method    = "kknn",
trControl = cv,
tuneGrid  = grid)
caret_knn_readability$times
View(blueprint_textdummy)
blueprint_textdummy <- recipe(x  = textdummy,
vars  = c(categorical, outcome, ID),
roles = c(rep('predictor',3), 'outcome', 'ID'))
caret_knn <- caret::train(blueprint_textdummy,
data      = rank_tr,
method    = "kknn",
trControl = cv,
tuneGrid  = grid)
View(rank_tr)
