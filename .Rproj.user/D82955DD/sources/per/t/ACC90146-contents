---
title: "EDLD 654 Homework 2"
author: "Diana DeWald"
date: "11/10/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(caret)
library(glmnet)
library(recipes)
library(finalfit)
library(cutpointr)
library(psych)
library(kableExtra)
library(vip)
```

## Part 1: Predicting a Categorical Outcome using Regularized Logistic Regression

```{r part1_prep, warning = FALSE}

tweet <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/hw1_tweet_final.csv',header=TRUE)

# Recipe for the tweet dataset

blueprint_tweet <- recipe(x  = tweet,
                          vars  = colnames(tweet),
                          roles = c('outcome',rep('predictor',772))) %>%
  step_dummy('month',one_hot=TRUE) %>% 
  step_harmonic('day',frequency=1,cycle_size=7, role='predictor') %>%
  step_harmonic('date',frequency=1,cycle_size=31,role='predictor') %>%
  step_harmonic('hour',frequency=1,cycle_size=24,role='predictor') %>%
  step_normalize(paste0('Dim',1:768)) %>%
  step_normalize(c('day_sin_1','day_cos_1',
                   'date_sin_1','date_cos_1',
                   'hour_sin_1','hour_cos_1')) %>%
  step_num2factor(sentiment,
                  transform = function(x) x + 1,
                  levels=c('Negative','Positive'))

  
    # Notice that I explicitly specified role=predictor when using
    # step_harmonic function. This assures that the newly derived sin and cos
    # variables has a defined role.
    # You need to do this otherwise caret::train function breaks.
    # caret_train requires every variable in the recipe to have a role
    
    # You can run the following code and make sure every variable has a defined 
    # role. If you want to experiment, remove the role=predictor argument
    # in the step_harmonic function, create the recipe again, and run the following
    # you will see that the new sin and cos variables have NA in the column role
    # and this breaks the caret::train function later.
  
    # Also, in the last line, we transform the outcome variable 'sentiment' to 
    # a factor with labels. 
    # This seems necessary for fitting logistic regression via caret::train

    # View(blueprint_tweet %>% prep() %>% summary)

```

### Task 1.1: split dataset into training and testing

```{r 1.1, warning = FALSE}
# Let the training data have the 80% of cases and the test data have the 20% of the cases.

set.seed(11102021)  # for reproducibility
  
twe      <- sample(1:nrow(tweet), round(nrow(tweet) * 0.8))
tweet_train  <- tweet[twe, ]
tweet_test  <- tweet[-twe, ]

```


### Task 1.2: 10-fold cross-validation without regularization

```{r 1.2, warning = FALSE}

set.seed(11152021) # for reproducibility

tweet_tr = tweet_train[sample(nrow(tweet_train)),]

  # Creating 10 folds with equal size

folds = cut(seq(1,nrow(tweet_tr)),breaks=10,labels=FALSE)
  
  # Creating the list for each fold 
tweet.indices <- vector('list',10)
      for(i in 1:10){
        tweet.indices[[i]] <- which(folds!=i)
      }
    

 cv <- trainControl(method    = "cv",
                   index           = tweet.indices,
                   classProbs      = TRUE,
                   summaryFunction = mnLogLoss)
    
      
# Train the model
  
caret_mod <- caret::train(blueprint_tweet, 
                          data      = tweet_tr, 
                          method    = "glm",
                          family    = 'binomial',
                          metric    = 'logLoss',
                          trControl = cv)

caret_mod

# Evaluate the model on the test data

# Predict the probabilities for the observations in the test dataset

predicted_test <- predict(caret_mod, tweet_test, type='prob')

head(predicted_test)

```
#### Will discuss this more in 1.5, but the model without regularization doesn't seem to be the best for predicting outcomes on the test dataset.

### Task 1.3: 10-fold cross-validation with ridge penalty
```{r 1.3 ridge, warning = FALSE}
 caret::getModelInfo()$glmnet$parameters

grid_ridge <- data.frame(alpha = 0, lambda = seq(0.01,2,.01)) 
# grid
  
  
  ridge <- caret::train(blueprint_tweet, 
                        data      = tweet_train, 
                        method    = "glmnet",
                        family = 'binomial', 
                        metric = 'logLoss',
                        trControl = cv,
                        tuneGrid  = grid_ridge)
  
# ridge$results


ridge$bestTune
  
plot(ridge)


```

The optimal lambda value for a lasso penalty was 0.15


### Task 1.4: 10-fold cross-validation with lasso penalty

```{r 1.4, warning = FALSE}

# Tune Grid  
  
  # we set the value of alpha to 1 for lasso regression
  
grid_lasso <- data.frame(alpha = 1, lambda = seq(0.01,3,.01)) 
    
# Train the model

lasso <- caret::train(blueprint_tweet, 
                        data      = tweet_train, 
                        method    = "glmnet", 
                        trControl = cv,
                        tuneGrid  = grid_lasso)

head(lasso$results)

lasso$bestTune
  
  
# comparing a model with different lambda values
grid_lasso.2 <- data.frame(alpha = 1, lambda = seq(0.001,0.015,.001)) 

# grid_lasso.2
    
# Train the model

lasso2 <- caret::train(blueprint_tweet, 
                        data      = tweet_train, 
                        method    = "glmnet", 
                        trControl = cv,
                        tuneGrid  = grid_lasso.2)

head(lasso2$results)

  
lasso2$bestTune
  
plot(lasso2)  

```
According to the second lasso model, the optimal lambda value for lasso penalty is 0.011.

### Task 1.5: Evaluate the performance of the models in 1.2, 1.3, and 1.4 on the test dataset.

```{r 1.5, warning = FALSE}

# Using the model from 1.2

predicted_test <- predict(caret_mod, tweet_test, type='prob')

dim(predicted_test)

pred_class <- ifelse(predicted_test$Positive>.5,1,0)

confusion <- table(tweet_test$sentiment,pred_class)

#found logLoss from results output

# Compute the AUC

cut.obj <- cutpointr(x     = predicted_test$Positive,
                     class = tweet_test$sentiment)

auc(cut.obj)


# overall accuracy (ACC) = (TPR +TNR)/# samples

ACC <- sum(diag(confusion))/300

ACC

# True Positive Rate

confusion[2,2]/(confusion[2,1]+confusion[2,2])

# True Negative Rate

confusion[1,1]/(confusion[1,1]+confusion[1,2])

# Precision

confusion[2,2]/(confusion[1,2]+confusion[2,2])




# Using model from 1.3
predicted_test.2 <- predict(ridge, tweet_test, type='prob')

dim(predicted_test.2)

pred_class.2 <- ifelse(predicted_test.2$Positive>.5,1,0)

confusion.2 <- table(tweet_test$sentiment ,pred_class.2)

# logloss
logLoss.2 <- c(0.5392010, 0.5182769, 0.4904754, 0.4756138, 0.4665867, 0.4606949, 0.4566203, 0.4537855, 0.4517692,  0.450343, 0.4493541, 0.4486941, 0.4482857, 0.4480724, 0.4480170, 0.4480945, 0.4482631, 0.4485184, 0.4488487, 0.4492259, 0.4496564, 0.4501227, 0.4506284, 0.4511546, 0.4517112, 0.4517112, 0.4522794, 0.4528656, 0.4534700, 0.4540772, 0.4547018, 0.4553286, 0.4559561, 0.4566002, 0.4572398, 0.4578771, 0.4585255, 0.4591727, 0.4598129, 0.460456, 0.4611074, 0.4617499, 0.4623852, 0.4630238, 0.4636675, 0.4643016,  0.4649271,  0.4655538,  0.4661882, 0.4668110, 0.4674243, 0.4680362, 0.4686519, 0.4692665, 0.4698724, 0.4704667, 0.4710606, 0.4716575, 0.4722545, 0.4728416,  0.4734191,  0.4739941,  0.4745662, 0.4751456, 0.4757147,  0.4762799,  0.4768326, 0.4773851, 0.4779345, 0.4784900, 0.4790387, 0.4795803, 0.4801148, 0.4806426, 0.4811687, 0.4816961, 0.4822281, 0.4827485, 0.4832655, 0.4837749, 0.4842775, 0.4847795, 0.4852804, 0.4857856, 0.4862866, 0.4867782, 0.4872694, 0.4877488, 0.4882270, 0.4887024, 0.4891778, 0.489656, 0.4901336, 0.4905998, 0.4910659, 0.4915245, 0.4919757, 0.4924286, 0.4928765, 0.4933266, 0.4937796, 0.4942294, 0.4946698, 0.4951095, 0.4955449, 0.4959709, 0.4963976, 0.4968217, 0.4972444, 0.4976694, 0.4980967, 0.4985178, 0.4989322, 0.4993460, 0.4997565, 0.5001577, 0.5005583, 0.5009590, 0.5013552, 0.5017532, 0.5021532, 0.5025546, 0.5029473, 0.5033359, 0.5037242, 0.5041096, 0.5044867, 0.5048617, 0.5052382, 0.5056100, 0.5059819,  0.5063553,  0.5067303, 0.5071056, 0.5074717, 0.5078351, 0.5081983, 0.5085598, 0.5089140, 0.5092639, 0.5096152, 0.5099654, 0.5103119, 0.5106596, 0.5110087, 0.5113590, 0.5117091, 0.5120506, 0.5123899, 0.5127285, 0.5130679, 0.5133995, 0.5137266, 0.5140533, 0.5143811, 0.5147056, 0.5150285, 0.515352, 0.5156774, 0.5160035, 0.5163299, 0.5166495, 0.5169663, 0.5172807, 0.5175961, 0.5179094, 0.5182167, 0.5185202, 0.5188239, 0.5191285, 0.5194302, 0.5197300, 0.5200306, 0.5203321, 0.5206345, 0.5209376, 0.5212396, 0.5215338, 0.5218272, 0.5221191, 0.5224117, 0.5227026, 0.5229880, 0.5232698, 0.5235510, 0.5238330, 0.5241146, 0.5243926, 0.5246706, 0.5249493, 0.5252286, 0.5255087, 0.5257894, 0.5260693, 0.5263426, 0.5266149, 0.5268849, 0.5271553, 0.5274264, 0.5276931, 0.5279562)

mean(logLoss.2)

# Compute the AUC

cut.obj.2 <- cutpointr(x     = predicted_test.2$Positive,
                     class = tweet_test$sentiment)

auc(cut.obj.2)


# overall accuracy (ACC) 

ACC.2 <- sum(diag(confusion.2))/300

ACC.2

# True Positive Rate

confusion.2[2,2]/(confusion.2[2,1]+confusion.2[2,2])

# True Negative Rate

confusion.2[1,1]/(confusion.2[1,1]+confusion.2[1,2])

# Precision

confusion.2[2,2]/(confusion.2[1,2]+confusion.2[2,2])




# Using model from 1.4
predicted_test.3 <- predict(lasso, tweet_test, type='prob')

dim(predicted_test.3)

pred_class.3 <- ifelse(predicted_test.3$Positive>.5,1,0)

confusion.3 <- table(tweet_test$sentiment ,pred_class.3)

# logloss
logLoss.3 <- c(0.4568330, 0.4726323, 0.4980172, 0.5215198, 0.5432886, 0.5658078, 0.587672, 0.6091400, 0.6292616, 0.6474315, 0.6623814, 0.671, 0.6783870, 0.6841476, 0.6899620, 0.6935984, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.693755,  0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558,  0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.69375, 0.6937558, 0.6937558, 0.693755, 0.6937558, 0.6937558, 0.6937558,  0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558,  0.693755, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.693755, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.693755, 0.6937558,  0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558,  0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.69375, 0.693, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558,  0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558,  0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558,   0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.693, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558, 0.6937558)

mean(logLoss.3)

# Compute the AUC

cut.obj.3 <- cutpointr(x     = predicted_test.3$Positive,
                     class = tweet_test$sentiment)

auc(cut.obj.3)


# overall accuracy (ACC) 

ACC.3 <- sum(diag(confusion.3))/300

ACC.3

# True Positive Rate

confusion.3[2,2]/(confusion.3[2,1]+confusion.3[2,2])

# True Negative Rate

confusion.3[1,1]/(confusion.3[1,1]+confusion.3[1,2])

# Precision

confusion.3[2,2]/(confusion.3[1,2]+confusion.3[2,2])





type <- c('Logistic Regression', 'Logistic Regression with Ridge Penalty', 'Logistic Regression with Lasso Penalty')
LL <- c(9.854, 0.491, 0.689)
AUC <- c(0.645, 0.869, 0.860)
ACC <- c(0.613, 0.797, 0.790)
TPR <- c(0.681, 0.801, 0.809)
TNR <- c(0.553, 0.792, 0.774)
PRE <- c(0.575, 0.774, 0.760)


model_compare <- data.frame(type, LL, AUC, ACC, TPR, TNR, PRE)

kbl(model_compare)
```
#### It appears that ridge regression is the best model to predict the sentiment of a tweet, so that is the model I'll use going forward.

### Task 1.6: Find and report the most important 10 predictors of sentiment and their coefficients. 

```{r 1.6, warning = FALSE}

# For the model you decided in 1.5, find and report the most important 10 predictors of sentiment and their coefficients. Briefly comment
coefs <- coef(lasso2$finalModel,lasso2$bestTune$lambda)

coefs.zero <- coefs[which(coefs[,1]==0),]
length(coefs.zero)

coefs.nonzero <- coefs[which(coefs[,1]!=0),]
length(coefs.nonzero)

ind   <- order(abs(coefs.nonzero),decreasing=T)
head(as.matrix(coefs.nonzero[ind[-1]]),10)

```
The 10 most important predictors of `tweet sentiment` are the `intercept`, normalized date (`date_sin_1`) and day (`day_cos_1`) coefficients, plus sentence processing variables `Dimensions 581, 588, 675, 104, 282, 351, and 263`.


### Task 1.7. Below are the two tweets I picked from my timeline. Use the model you decided in Task 1.5 to predict a probability that the sentiment being positive for these tweets. 

#### Used Rstudio cloud for this--according to the results there, the prediction model guessed that tweet 1 had a probability of 0.554 for the sentiment being positive, and tweet 2 had a probability of 0.318 for the sentiment being positive.

### Task 1.8. Let’s do an experiment and test whether or not the model is biased against certain groups when detecting sentiment of a given text. Below you will find 10 hypothetical tweets with an identical structure. The only thing that changes from tweet to tweet is the subject

```{r 1.8, warning = FALSE}
group <- c('Muslims', 'Jews', 'Christians', 		'Atheists', 'Buddhists', 'Turkish people','French people', 'American people',	'Japanese people', 'Russian people')

judgement <- c('are so annoying!', 'are so annoying!', 'are so annoying!', 'are so annoying!', 'are so annoying!', 'are so annoying!', 'are so annoying!', 'are so annoying!', 'are so annoying!', 'are so annoying!')

Prob_Sentiment=Positive <- c(0.701, 0.596, 0.612, 0.659, 0.610, 0.692, 0.612, 0.746, 0.579, 0.729)

model_predict <- data.frame(group, judgement, Prob_Sentiment=Positive)

kbl(model_predict)

```

It does appear that the model tends to be biased toward positive sentiment at how annoying certain groups are, in particular, Muslims, American people, and Russian people. On the other hand, tweets least likely to have positive sentiment for calling groups annoying involve judgments about Japanese people, Jews, Buddhists, Christians, and French people. Intermediate groups in terms of 'annoying' judgments are Turkish people and Atheists.

## Part 2: Predicting a Continuous Outcome using Regularized Linear Regression

### Task 2.1: Load dataset and check for missingness

```{r 2.1, warning = FALSE}

# Loading the dataset
oregon <- read.csv('https://raw.githubusercontent.com/uo-datasci-specialization/c4-ml-fall-2021/main/content/post/hw2/data/hw1_oregon_final.csv',header=TRUE)

# Recipe for the oregon dataset

  outcome <- 'score'
  
  id      <- 'id'

  categorical <- c('sex','ethnic_cd','tst_bnch','migrant_ed_fg','ind_ed_fg',
                   'sp_ed_fg','tag_ed_fg','econ_dsvntg','stay_in_dist',
                   'stay_in_schl','dist_sped','trgt_assist_fg',
                   'ayp_dist_partic','ayp_schl_partic','ayp_dist_prfrm',
                   'ayp_schl_prfrm','rc_dist_partic','rc_schl_partic',
                   'rc_dist_prfrm','rc_schl_prfrm','grp_rpt_dist_partic',
                   'grp_rpt_schl_partic','grp_rpt_dist_prfrm','grp_rpt_schl_prfrm')

  numeric <- c('enrl_grd')

  cyclic <- c('date','month')


blueprint_oregon <- recipe(x     = oregon,
                    vars  = c(outcome,categorical,numeric,cyclic),
                    roles = c('outcome',rep('predictor',27))) %>%
  step_indicate_na(all_of(categorical),all_of(numeric)) %>%
  step_zv(all_numeric()) %>%
  step_impute_mean(all_of(numeric)) %>%
  step_impute_mode(all_of(categorical)) %>%
  step_harmonic('date',frequency=1,cycle_size=31,role='predictor') %>%
  step_harmonic('month',frequency=1,cycle_size=12,role='predictor') %>%
  step_ns('enrl_grd',deg_free=3) %>%
  step_normalize(c(paste0(numeric,'_ns_1'),paste0(numeric,'_ns_2'),paste0(numeric,'_ns_3'))) %>%
  step_normalize(c("date_sin_1","date_cos_1","month_sin_1","month_cos_1")) %>%
  step_dummy(all_of(categorical),one_hot=TRUE) %>%
  step_rm(c('date','month'))
    
# View(blueprint_oregon %>% prep() %>% summary)

# removing missingness
missing_ <- ff_glimpse(oregon)$Continuous
missing_$missing_percent
# flag_na <- which(as.numeric(missing_$missing_percent) > 75)
# oregon <- oregon[,-flag_na]

```


### Task 2.2: Split dataset into training and testing
```{r 2.2, warning = FALSE}

# Let the training data have the 80% of cases and the test data have the 20% of the cases.

set.seed(11172021)  # for reproducibility
  
ore      <- sample(1:nrow(oregon), round(nrow(oregon) * 0.8))
oregon_train  <- oregon[ore, ]
oregon_test  <- oregon[-ore, ]

```

### Task 2.3: 10-fold cross-validation without regularization 
```{r 2.3, warning = FALSE}

set.seed(11172021) # for reproducibility

oregon_tr = oregon_train[sample(nrow(oregon_train)),]

  # Creating 10 folds with equal size

folds = cut(seq(1,nrow(oregon_tr)),breaks=10,labels=FALSE)
  
  # Creating the list for each fold 
oregon.indices <- vector('list',10)
      for(i in 1:10){
        oregon.indices[[i]] <- which(folds!=i)
      }
    

 cv_oregon <- trainControl(method    = "cv",
                   index           = oregon.indices)
    
      
# Train the model
  
caret_mod_oregon <- caret::train(blueprint_oregon, 
                        data      = oregon_train, 
                        method    = "lm",
                       trControl = cv_oregon)

caret_mod_oregon

# Evaluate the model on the training dataset
predicted_tr <- predict(caret_mod_oregon, oregon_train)

rsq_tr <- cor(oregon_train$score,predicted_tr)^2
rsq_tr

mae_tr <- mean(abs(oregon_train$score - predicted_tr))
mae_tr

rmse_tr <- sqrt(mean((oregon_train$score - predicted_tr)^2))
rmse_tr

# Evaluating the model on the test dataset
predicted_te <- predict(caret_mod_oregon, oregon_test)

rsq_te <- cor(oregon_test$score,predicted_te)^2
rsq_te

mae_te <- mean(abs(oregon_test$score - predicted_te))
mae_te

rmse_te <- sqrt(mean((oregon_test$score - predicted_te)^2))
rmse_te
```

### Task 2.4: 10-fold cross-validation with ridge penalty

``` {r 2.4, warning = FALSE}
caret::getModelInfo()$glmnet$parameters

grid_ridge_oregon <- data.frame(alpha = 0, lambda = seq(0.01,2,.01)) 
  
  
ridge_oregon <- caret::train(blueprint_oregon, 
                        data      = oregon_train, 
                        method    = "glmnet",
                        trControl = cv_oregon,
                        tuneGrid  = grid_ridge_oregon)
  
# ridge$results


ridge_oregon$bestTune
  
# plot(ridge_oregon)

# trying different lambda value
grid_ridge_oregon.2 <- data.frame(alpha = 0, lambda = seq(0.01,10,.1)) 

ridge_oregon.2 <- caret::train(blueprint_oregon, 
                        data      = oregon_train, 
                        method    = "glmnet",
                        trControl = cv_oregon,
                        tuneGrid  = grid_ridge_oregon.2)

plot(ridge_oregon.2)

# Evaluating performance on test dataset
predict_te_ridge <- predict(ridge_oregon.2, oregon_test)

rsq_te <- cor(oregon_test$score,predict_te_ridge)^2
rsq_te


mae_te <- mean(abs(oregon_test$score - predict_te_ridge))
mae_te

rmse_te <- sqrt(mean((oregon_test$score - predict_te_ridge)^2))
rmse_te


```


### Task 2.5: 10-fold cross-validation with lasso penalty
```{r 2.5, warning = FALSE}

grid_lasso_oregon <- data.frame(alpha = 1, lambda = seq(0.001,0.015,.001)) 

grid_lasso_oregon

oregon_lasso <- caret::train(blueprint_oregon, 
                        data      = oregon_train, 
                        method    = "glmnet", 
                        trControl = cv_oregon,
                        tuneGrid  = grid_lasso_oregon)

# head(oregon_lasso$results)


grid_lasso_oregon.2 <- data.frame(alpha = 1, lambda = seq(0.005,5,.001)) 
    
# Evaluating model with different lambda

  oregon_lasso2 <- caret::train(blueprint_oregon, 
                        data      = oregon_train, 
                        method    = "glmnet", 
                        trControl = cv_oregon,
                        tuneGrid  = grid_lasso_oregon.2)

head(oregon_lasso2$results)

  
oregon_lasso2$bestTune
  
plot(oregon_lasso2)  

# Evaluating model on test data

predict_te_lasso <- predict(oregon_lasso2, oregon_test)

rsq_te <- cor(oregon_test$score,predict_te_lasso)^2
rsq_te

mae_te <- mean(abs(oregon_test$score - predict_te_lasso))
mae_te

rmse_te <- sqrt(mean((oregon_test$score - predict_te_lasso)^2))
rmse_te
```

### Task 2.6: Summarize performance of models on test dataset

```{r 2.6, warning = FALSE}

# Making the table
type <- c('Linear Regression', 'Linear Regression with Ridge Penalty', 'Linear Regression with Lasso Penalty')
RMSE <- c(88.9664, 88.9710, 88.9790)
MAE <- c(69.1845, 69.1961, 69.1945)
R_sq <- c(0.4014, 0.40129, 0.4012)

model_compare_oregon <- data.frame(type, RMSE, MAE, R_sq)

kbl(model_compare_oregon)
```


### Task 2.7: Find and report the most important 10 predictors of test score and their coefficients. 

#### In this case, it appears that the 10-fold model without regularization is the best fit, since it has the lowest RMSE and MAE values. Thus, lasso or ridge penalties will cause overfitting.

```{r 2.7, warning = FALSE}

vip(caret_mod_oregon, num_features = 10, geom = "point") + 
  theme_bw()

```

#### The 10 most important predictors of `test score` are normalized month (`month_cos_1`) and date (`date_sin_1`), several ethnicity factors (`ethnic_cd_H`, `ethnic_cd_Black`, `ethnic_cd_A`, `ethnic_cd_P`, `ethnic_cd_I`), `tag_ed_fg_n`, `sp_ed_fg_N`, and an SES factor (`econv_disvntg_N`).